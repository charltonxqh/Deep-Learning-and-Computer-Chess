{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning and Computer Chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import chess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEN</th>\n",
       "      <th>Evaluation</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...</td>\n",
       "      <td>5.2</td>\n",
       "      <td>White is winning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...</td>\n",
       "      <td>5.2</td>\n",
       "      <td>White is winning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2k4r/ppp2p2/2b2B2/7p/6pP/2P4P/PP3Nq1/R5K1 w - ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Even</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 FEN  Evaluation  \\\n",
       "0  r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...         5.2   \n",
       "1  r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...         5.2   \n",
       "2  4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...         0.0   \n",
       "3  4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...         0.0   \n",
       "4  2k4r/ppp2p2/2b2B2/7p/6pP/2P4P/PP3Nq1/R5K1 w - ...         0.0   \n",
       "\n",
       "           Category  \n",
       "0  White is winning  \n",
       "1  White is winning  \n",
       "2              Even  \n",
       "3              Even  \n",
       "4              Even  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../output/evaluations.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76916, 3)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FEN           0\n",
       "Evaluation    0\n",
       "Category      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fen</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...</td>\n",
       "      <td>5.2</td>\n",
       "      <td>White is winning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...</td>\n",
       "      <td>5.2</td>\n",
       "      <td>White is winning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2k4r/ppp2p2/2b2B2/7p/6pP/2P4P/PP3Nq1/R5K1 w - ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Even</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 fen  evaluation  \\\n",
       "0  r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...         5.2   \n",
       "1  r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...         5.2   \n",
       "2  4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...         0.0   \n",
       "3  4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...         0.0   \n",
       "4  2k4r/ppp2p2/2b2B2/7p/6pP/2P4P/PP3Nq1/R5K1 w - ...         0.0   \n",
       "\n",
       "           category  \n",
       "0  White is winning  \n",
       "1  White is winning  \n",
       "2              Even  \n",
       "3              Even  \n",
       "4              Even  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'FEN': 'fen', 'Evaluation': 'evaluation', 'Category': 'category'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def king_safety_vector(board, king_color):\n",
    "    king_square = board.king(king_color)\n",
    "\n",
    "    # King File Openness\n",
    "    king_file = chess.square_file(king_square)\n",
    "    king_file_open = 1 if all(board.piece_at(chess.square(king_file, rank)) is None for rank in range(8)) else 0\n",
    "\n",
    "    # Pawn Shield\n",
    "    pawn_shield = [\n",
    "        1 if board.piece_at(chess.square(king_file + offset, 1 if king_color == chess.WHITE else 6)) and \n",
    "        board.piece_at(chess.square(king_file + offset, 1 if king_color == chess.WHITE else 6)).symbol().lower() == 'p' else 0 \n",
    "        for offset in [-1, 0, 1] if 0 <= king_file + offset < 8\n",
    "    ]\n",
    "    while len(pawn_shield) < 3:\n",
    "        pawn_shield.append(0)\n",
    "\n",
    "    # Castling Status\n",
    "    castling_rights = [\n",
    "        1 if board.has_kingside_castling_rights(king_color) else 0,\n",
    "        1 if board.has_queenside_castling_rights(king_color) else 0,\n",
    "    ]\n",
    "\n",
    "    # Enemy Pieces Attacking King\n",
    "    enemy_color = chess.BLACK if king_color == chess.WHITE else chess.WHITE\n",
    "    # king_attackers = len(board.attackers(enemy_color, king_square))\n",
    "\n",
    "    # King Centralization\n",
    "    king_centralization = 1 if king_file in [2, 3, 4, 5] else 0\n",
    "\n",
    "    # King Escape Squares\n",
    "\n",
    "\n",
    "    # Piece Blocking Check\n",
    "    if board.is_check():\n",
    "    # Find all possible blocking squares (between attacker and king)\n",
    "        attack_moves = [\n",
    "            move for move in board.legal_moves\n",
    "            if move.to_square in chess.SquareSet.between(board.king(board.turn), board.king(not board.turn))\n",
    "        ]\n",
    "        can_block_check = 1 if attack_moves else 0\n",
    "    else:\n",
    "        can_block_check = 0\n",
    "\n",
    "\n",
    "    # King Mobility\n",
    "    # king_mobility = min(len(list(board.legal_moves)), 5)\n",
    "\n",
    "    # Opposing Queen Presence\n",
    "    queen_presence = 1 if any(piece.piece_type == chess.QUEEN and piece.color == enemy_color for piece in board.piece_map().values()) else 0\n",
    "\n",
    "    king_safety_vector = [\n",
    "        king_file_open,\n",
    "        *pawn_shield,\n",
    "        *castling_rights,\n",
    "        king_centralization,\n",
    "        can_block_check,\n",
    "        queen_presence\n",
    "    ]\n",
    "\n",
    "    return king_safety_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def king_safety(fen):\n",
    "    board = chess.Board(fen)\n",
    "    \n",
    "    white_king_vector = king_safety_vector(board, chess.WHITE)\n",
    "    black_king_vector = king_safety_vector(board, chess.BLACK)\n",
    "\n",
    "    return white_king_vector + black_king_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def material(fen):\n",
    "    board = chess.Board(fen)\n",
    "\n",
    "    piece_types = [chess.PAWN, chess.KNIGHT, chess.BISHOP, chess.ROOK, chess.QUEEN, chess.KING]\n",
    "\n",
    "    white_pieces = [len(board.pieces(piece, chess.WHITE)) for piece in piece_types]\n",
    "    black_pieces = [len(board.pieces(piece, chess.BLACK)) for piece in piece_types]\n",
    "\n",
    "    material_vector = np.array(white_pieces + black_pieces, dtype=np.float32)\n",
    "\n",
    "    return material_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def space(fen):\n",
    "    board = chess.Board(fen)\n",
    "\n",
    "    white_space, black_space = 0, 0\n",
    "\n",
    "    white_pawns = board.pieces(chess.PAWN, chess.WHITE)\n",
    "    black_pawns = board.pieces(chess.PAWN, chess.BLACK)\n",
    "\n",
    "    for square in white_pawns:\n",
    "        rank = chess.square_rank(square)\n",
    "        if rank >= 4:\n",
    "            white_space += 1\n",
    "\n",
    "    for square in black_pawns:\n",
    "        rank = chess.square_rank(square)\n",
    "        if rank <= 3:\n",
    "            black_space += 1\n",
    "\n",
    "    white_mobility = len(list(board.legal_moves)) if board.turn == chess.WHITE else 0\n",
    "    black_mobility = len(list(board.legal_moves)) if board.turn == chess.BLACK else 0\n",
    "\n",
    "    total_white_space = white_space + white_mobility\n",
    "    total_black_space = black_space + black_mobility\n",
    "\n",
    "    return [total_white_space, total_black_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pawn_structure(fen):\n",
    "    board = chess.Board(fen)\n",
    "\n",
    "    def count_isolated_pawns(pawns):\n",
    "        isolated_pawns = 0\n",
    "        for file in range(8):\n",
    "            if pawns & chess.BB_FILES[file]:\n",
    "                left_file = file - 1\n",
    "                right_file = file + 1\n",
    "                if (left_file >= 0 and pawns & chess.BB_FILES[left_file]) or (right_file <= 7 and pawns & chess.BB_FILES[right_file]):\n",
    "                    continue\n",
    "                isolated_pawns += 1\n",
    "\n",
    "        return isolated_pawns\n",
    "    \n",
    "    def count_doubled_pawns(pawns):\n",
    "        doubled_pawns = 0\n",
    "\n",
    "        for file in range(8):\n",
    "            file_pawns = pawns & chess.BB_FILES[file]\n",
    "            pawn_ranks = [chess.square_rank(square) for square in chess.SquareSet(file_pawns)]\n",
    "\n",
    "            pawn_ranks.sort()\n",
    "\n",
    "            for i in range(len(pawn_ranks) - 1):\n",
    "                if pawn_ranks[i] + 1 == pawn_ranks[i + 1]:\n",
    "                    doubled_pawns += 1\n",
    "                    break\n",
    "\n",
    "        return doubled_pawns\n",
    "    \n",
    "    def count_backward_pawns(pawns, color):\n",
    "        backward_pawns = 0\n",
    "\n",
    "        for file in range(8):\n",
    "            file_mask = chess.BB_FILES[file]\n",
    "            pawns_in_file = pawns & file_mask\n",
    "\n",
    "            if pawns_in_file:\n",
    "                forward_pawn = file_mask >> 8 if color == chess.WHITE else file_mask << 8\n",
    "\n",
    "                left_file = file - 1 if file > 0 else None\n",
    "                right_file = file + 1 if file < 7 else None\n",
    "\n",
    "                left_pawns = pawns & chess.BB_FILES[left_file] if left_file is not None else 0\n",
    "                right_pawns = pawns & chess.BB_FILES[right_file] if right_file is not None else 0\n",
    "\n",
    "                if not forward_pawn and ((left_pawns & left_pawns >> 8 if color == chess.WHITE else left_pawns << 8) or (right_pawns & right_pawns >> 8 if color == chess.WHITE else right_pawns << 8)):\n",
    "                    backward_pawns += 1\n",
    "        \n",
    "        return backward_pawns\n",
    "\n",
    "    def count_passed_pawns(pawns, enemy_pawns):\n",
    "        passed_pawns = 0\n",
    "\n",
    "        for file in range(8):\n",
    "            if pawns & chess.BB_FILES[file]:\n",
    "                front_mask = enemy_pawns & chess.BB_FILES[file]\n",
    "                if not front_mask:\n",
    "                    passed_pawns += 1\n",
    "        \n",
    "        return passed_pawns\n",
    "\n",
    "    def count_pawn_islands(pawns):\n",
    "        pawn_islands = 0\n",
    "        prev_file_pawn_ranks = set()\n",
    "\n",
    "        for file in range(8):\n",
    "            file_pawns = pawns & chess.BB_FILES[file]\n",
    "            pawn_ranks = sorted([chess.square_rank(square) for square in chess.SquareSet(file_pawns)])\n",
    "\n",
    "            if not pawn_ranks:\n",
    "                prev_file_pawn_ranks = set()\n",
    "                continue\n",
    "\n",
    "            rank_islands = 1\n",
    "            for i in range(len(pawn_ranks) - 1):\n",
    "                if pawn_ranks[i + 1] > pawn_ranks[i] + 1:\n",
    "                    rank_islands += 1\n",
    "\n",
    "            if prev_file_pawn_ranks:\n",
    "                connected = False\n",
    "                for rank in pawn_ranks:\n",
    "                    for prev_rank in prev_file_pawn_ranks:\n",
    "                        if abs(rank - prev_rank) <= 1:\n",
    "                            connected = True\n",
    "                            break\n",
    "                    if connected:\n",
    "                        break\n",
    "\n",
    "                if connected:\n",
    "                    rank_islands -= 1\n",
    "\n",
    "            pawn_islands += rank_islands\n",
    "            prev_file_pawn_ranks = set(pawn_ranks)\n",
    "\n",
    "        return pawn_islands\n",
    "\n",
    "    white_pawns = board.pieces(chess.PAWN, chess.WHITE)\n",
    "    black_pawns = board.pieces(chess.PAWN, chess.BLACK)\n",
    "\n",
    "    white_enemy_pawns = black_pawns\n",
    "    black_enemy_pawns = white_pawns\n",
    "\n",
    "    features = [\n",
    "        count_isolated_pawns(white_pawns),\n",
    "        count_doubled_pawns(white_pawns),\n",
    "        count_backward_pawns(white_pawns, chess.WHITE),\n",
    "        count_passed_pawns(white_pawns, white_enemy_pawns),\n",
    "        count_pawn_islands(white_pawns),\n",
    "        count_isolated_pawns(black_pawns),\n",
    "        count_doubled_pawns(black_pawns),\n",
    "        count_backward_pawns(black_pawns, chess.BLACK),\n",
    "        count_passed_pawns(black_pawns, black_enemy_pawns),\n",
    "        count_pawn_islands(black_pawns)\n",
    "    ]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def development(fen):\n",
    "    board = chess.Board(fen)\n",
    "\n",
    "    development_vector = []\n",
    "\n",
    "    minor_pieces = {\n",
    "        \"Knight1_W\": chess.B1,\n",
    "        \"Knight2_W\": chess.G1,\n",
    "        \"Bishop1_W\": chess.C1,\n",
    "        \"Bishop2_W\": chess.F1,\n",
    "        \"Knight1_B\": chess.B8,\n",
    "        \"Knight2_B\": chess.G8,\n",
    "        \"Bishop1_B\": chess.C8,\n",
    "        \"Bishop2_B\": chess.F8\n",
    "    }\n",
    "\n",
    "    for piece_name, start_square in minor_pieces.items():\n",
    "        piece_type = chess.KNIGHT if \"Knight\" in piece_name else chess.BISHOP\n",
    "        color = chess.WHITE if \"_W\" in piece_name else chess.BLACK\n",
    "        piece_moved = 1 if board.piece_at(start_square) is None or board.piece_at(start_square).color != color else 0\n",
    "        development_vector.append(piece_moved)\n",
    "\n",
    "    center_squares = [chess.D4, chess.E4, chess.D5, chess.E5]\n",
    "    white_center_control = sum(1 for square in center_squares if board.piece_at(square) is not None and board.piece_at(square).color == chess.WHITE)\n",
    "    black_center_control = sum(1 for square in center_squares if board.piece_at(square) is not None and board.piece_at(square).color == chess.BLACK)\n",
    "    development_vector.append(white_center_control)\n",
    "    development_vector.append(black_center_control)\n",
    "\n",
    "    return development_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fen_to_bitstring(fen):\n",
    "    board = chess.Board(fen)\n",
    "\n",
    "    piece_types = {\n",
    "        'P': 0,\n",
    "        'N': 1,\n",
    "        'B': 2,\n",
    "        'R': 3,\n",
    "        'Q': 4,\n",
    "        'K': 5,\n",
    "        'p': 6,\n",
    "        'n': 7,\n",
    "        'b': 8,\n",
    "        'r': 9,\n",
    "        'q': 10,\n",
    "        'k': 11\n",
    "    }\n",
    "\n",
    "    bitboard = np.zeros(768, dtype=int)\n",
    "\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            piece_index = piece_types[piece.symbol()]\n",
    "            bitboard[square * 12 + piece_index] = 1\n",
    "\n",
    "    # Side to move\n",
    "    side_to_move = 1 if board.turn == chess.WHITE else 0\n",
    "\n",
    "    # Castling rights\n",
    "    castling_rights = [\n",
    "        1 if board.has_kingside_castling_rights(chess.WHITE) else 0,\n",
    "        1 if board.has_queenside_castling_rights(chess.WHITE) else 0,\n",
    "        1 if board.has_kingside_castling_rights(chess.BLACK) else 0,\n",
    "        1 if board.has_queenside_castling_rights(chess.BLACK) else 0\n",
    "    ]\n",
    "\n",
    "    # Combine everything into a 773-bit string\n",
    "    bitstring = np.concatenate((bitboard, [side_to_move] + castling_rights))\n",
    "\n",
    "    return bitstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fen</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>category</th>\n",
       "      <th>bitstring</th>\n",
       "      <th>king_safety</th>\n",
       "      <th>material</th>\n",
       "      <th>space</th>\n",
       "      <th>pawn_structure</th>\n",
       "      <th>development</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...</td>\n",
       "      <td>5.2</td>\n",
       "      <td>White is winning</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[5.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...</td>\n",
       "      <td>[1, 5]</td>\n",
       "      <td>[2, 0, 0, 2, 4, 2, 0, 0, 1, 4]</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...</td>\n",
       "      <td>5.2</td>\n",
       "      <td>White is winning</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[5.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...</td>\n",
       "      <td>[1, 5]</td>\n",
       "      <td>[2, 0, 0, 2, 4, 2, 0, 0, 1, 4]</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Even</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...</td>\n",
       "      <td>[31, 0]</td>\n",
       "      <td>[2, 0, 0, 1, 3, 0, 0, 0, 1, 2]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 1, 1, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Even</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...</td>\n",
       "      <td>[31, 0]</td>\n",
       "      <td>[2, 0, 0, 1, 3, 0, 0, 0, 1, 2]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 1, 1, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2k4r/ppp2p2/2b2B2/7p/6pP/2P4P/PP3Nq1/R5K1 w - ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Even</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[5.0, 1.0, 1.0, 1.0, 0.0, 1.0, 6.0, 0.0, 1.0, ...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[1, 1, 0, 0, 2, 0, 0, 0, 2, 3]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 fen  evaluation  \\\n",
       "0  r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...         5.2   \n",
       "1  r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...         5.2   \n",
       "2  4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...         0.0   \n",
       "3  4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...         0.0   \n",
       "4  2k4r/ppp2p2/2b2B2/7p/6pP/2P4P/PP3Nq1/R5K1 w - ...         0.0   \n",
       "\n",
       "           category                                          bitstring  \\\n",
       "0  White is winning  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  White is winning  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2              Even  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3              Even  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4              Even  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                         king_safety  \\\n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...   \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                            material    space  \\\n",
       "0  [5.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...   [1, 5]   \n",
       "1  [5.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...   [1, 5]   \n",
       "2  [4.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...  [31, 0]   \n",
       "3  [4.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...  [31, 0]   \n",
       "4  [5.0, 1.0, 1.0, 1.0, 0.0, 1.0, 6.0, 0.0, 1.0, ...   [0, 1]   \n",
       "\n",
       "                   pawn_structure                     development  \n",
       "0  [2, 0, 0, 2, 4, 2, 0, 0, 1, 4]  [1, 1, 1, 0, 1, 1, 1, 1, 1, 0]  \n",
       "1  [2, 0, 0, 2, 4, 2, 0, 0, 1, 4]  [1, 1, 1, 0, 1, 1, 1, 1, 1, 0]  \n",
       "2  [2, 0, 0, 1, 3, 0, 0, 0, 1, 2]  [1, 0, 1, 1, 1, 1, 1, 1, 2, 1]  \n",
       "3  [2, 0, 0, 1, 3, 0, 0, 0, 1, 2]  [1, 0, 1, 1, 1, 1, 1, 1, 2, 1]  \n",
       "4  [1, 1, 0, 0, 2, 0, 0, 0, 2, 3]  [1, 0, 1, 1, 1, 1, 0, 1, 0, 0]  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bitstring'] = df['fen'].apply(lambda x: fen_to_bitstring(x))\n",
    "df['king_safety'] = df['fen'].apply(lambda x: king_safety(x))\n",
    "df['material'] = df['fen'].apply(lambda x: material(x))\n",
    "df['space'] = df['fen'].apply(lambda x: space(x))\n",
    "df['pawn_structure'] = df['fen'].apply(lambda x: pawn_structure(x))\n",
    "df['development'] = df['fen'].apply(lambda x: development(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(773,)\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n",
      "[5. 0. 1. 1. 1. 1. 4. 1. 0. 1. 1. 1.]\n",
      "[1, 5]\n",
      "[2, 0, 0, 2, 4, 2, 0, 0, 1, 4]\n",
      "[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(df['bitstring'][0].shape)\n",
    "print(df['king_safety'][0])\n",
    "print(df['material'][0])\n",
    "print(df['space'][0])\n",
    "print(df['pawn_structure'][0])\n",
    "print(df['development'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fen</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>category</th>\n",
       "      <th>bitstring</th>\n",
       "      <th>king_safety</th>\n",
       "      <th>material</th>\n",
       "      <th>space</th>\n",
       "      <th>pawn_structure</th>\n",
       "      <th>development</th>\n",
       "      <th>normalized_material</th>\n",
       "      <th>normalized_space</th>\n",
       "      <th>normalized_pawn_structure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...</td>\n",
       "      <td>5.2</td>\n",
       "      <td>White is winning</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[5.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...</td>\n",
       "      <td>[1, 5]</td>\n",
       "      <td>[2, 0, 0, 2, 4, 2, 0, 0, 1, 4]</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]</td>\n",
       "      <td>[1.0, 0.0, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.0, ...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>[0.5, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.25,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...</td>\n",
       "      <td>5.2</td>\n",
       "      <td>White is winning</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[5.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...</td>\n",
       "      <td>[1, 5]</td>\n",
       "      <td>[2, 0, 0, 2, 4, 2, 0, 0, 1, 4]</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]</td>\n",
       "      <td>[1.0, 0.0, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.0, ...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>[0.5, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.25,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Even</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...</td>\n",
       "      <td>[31, 0]</td>\n",
       "      <td>[2, 0, 0, 1, 3, 0, 0, 0, 1, 2]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 1, 1, 2, 1]</td>\n",
       "      <td>[1.0, 0.0, 0.25, 0.25, 0.25, 0.25, 1.0, 0.25, ...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>[0.6666666666666666, 0.0, 0.0, 0.3333333333333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Even</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...</td>\n",
       "      <td>[31, 0]</td>\n",
       "      <td>[2, 0, 0, 1, 3, 0, 0, 0, 1, 2]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 1, 1, 2, 1]</td>\n",
       "      <td>[1.0, 0.0, 0.25, 0.25, 0.25, 0.25, 1.0, 0.25, ...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>[0.6666666666666666, 0.0, 0.0, 0.3333333333333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2k4r/ppp2p2/2b2B2/7p/6pP/2P4P/PP3Nq1/R5K1 w - ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Even</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[5.0, 1.0, 1.0, 1.0, 0.0, 1.0, 6.0, 0.0, 1.0, ...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[1, 1, 0, 0, 2, 0, 0, 0, 2, 3]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 1, 0, 0]</td>\n",
       "      <td>[0.8333334, 0.16666667, 0.16666667, 0.16666667...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>[0.3333333333333333, 0.3333333333333333, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 fen  evaluation  \\\n",
       "0  r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...         5.2   \n",
       "1  r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...         5.2   \n",
       "2  4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...         0.0   \n",
       "3  4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...         0.0   \n",
       "4  2k4r/ppp2p2/2b2B2/7p/6pP/2P4P/PP3Nq1/R5K1 w - ...         0.0   \n",
       "\n",
       "           category                                          bitstring  \\\n",
       "0  White is winning  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  White is winning  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2              Even  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3              Even  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4              Even  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                         king_safety  \\\n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...   \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                            material    space  \\\n",
       "0  [5.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...   [1, 5]   \n",
       "1  [5.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...   [1, 5]   \n",
       "2  [4.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...  [31, 0]   \n",
       "3  [4.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...  [31, 0]   \n",
       "4  [5.0, 1.0, 1.0, 1.0, 0.0, 1.0, 6.0, 0.0, 1.0, ...   [0, 1]   \n",
       "\n",
       "                   pawn_structure                     development  \\\n",
       "0  [2, 0, 0, 2, 4, 2, 0, 0, 1, 4]  [1, 1, 1, 0, 1, 1, 1, 1, 1, 0]   \n",
       "1  [2, 0, 0, 2, 4, 2, 0, 0, 1, 4]  [1, 1, 1, 0, 1, 1, 1, 1, 1, 0]   \n",
       "2  [2, 0, 0, 1, 3, 0, 0, 0, 1, 2]  [1, 0, 1, 1, 1, 1, 1, 1, 2, 1]   \n",
       "3  [2, 0, 0, 1, 3, 0, 0, 0, 1, 2]  [1, 0, 1, 1, 1, 1, 1, 1, 2, 1]   \n",
       "4  [1, 1, 0, 0, 2, 0, 0, 0, 2, 3]  [1, 0, 1, 1, 1, 1, 0, 1, 0, 0]   \n",
       "\n",
       "                                 normalized_material normalized_space  \\\n",
       "0  [1.0, 0.0, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.0, ...       [0.0, 1.0]   \n",
       "1  [1.0, 0.0, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.0, ...       [0.0, 1.0]   \n",
       "2  [1.0, 0.0, 0.25, 0.25, 0.25, 0.25, 1.0, 0.25, ...       [1.0, 0.0]   \n",
       "3  [1.0, 0.0, 0.25, 0.25, 0.25, 0.25, 1.0, 0.25, ...       [1.0, 0.0]   \n",
       "4  [0.8333334, 0.16666667, 0.16666667, 0.16666667...       [0.0, 1.0]   \n",
       "\n",
       "                           normalized_pawn_structure  \n",
       "0  [0.5, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.25,...  \n",
       "1  [0.5, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.25,...  \n",
       "2  [0.6666666666666666, 0.0, 0.0, 0.3333333333333...  \n",
       "3  [0.6666666666666666, 0.0, 0.0, 0.3333333333333...  \n",
       "4  [0.3333333333333333, 0.3333333333333333, 0.0, ...  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df['normalized_material'] = df['material'].apply(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)).flatten())\n",
    "df['normalized_space'] = df['space'].apply(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)).flatten())\n",
    "df['normalized_pawn_structure'] = df['pawn_structure'].apply(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)).flatten())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(773,)\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n",
      "[1.  0.  0.2 0.2 0.2 0.2 0.8 0.2 0.  0.2 0.2 0.2]\n",
      "[0. 1.]\n",
      "[0.5  0.   0.   0.5  1.   0.5  0.   0.   0.25 1.  ]\n",
      "[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(df['bitstring'][0].shape)\n",
    "print(df['king_safety'][0])\n",
    "print(df['normalized_material'][0])\n",
    "print(df['normalized_space'][0])\n",
    "print(df['normalized_pawn_structure'][0])\n",
    "print(df['development'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['king_safety'] = df['king_safety'].apply(lambda x: np.array(x))\n",
    "df['normalized_material'] = df['normalized_material'].apply(lambda x: np.array(x))\n",
    "df['normalized_space'] = df['normalized_space'].apply(lambda x: np.array(x))\n",
    "df['normalized_pawn_structure'] = df['normalized_pawn_structure'].apply(lambda x: np.array(x))\n",
    "df['development'] = df['development'].apply(lambda x: np.array(x))\n",
    "\n",
    "df['vector'] = df.apply(lambda row: np.concatenate([\n",
    "    row['king_safety'],\n",
    "    row['normalized_material'],\n",
    "    row['normalized_space'],\n",
    "    row['normalized_pawn_structure'],\n",
    "    row['development']\n",
    "]), axis=1)\n",
    "\n",
    "df['global'] = df.apply(lambda row: np.concatenate([\n",
    "    row['normalized_material'],\n",
    "    row['normalized_space'],\n",
    "    row['development']\n",
    "]), axis=1)\n",
    "\n",
    "df['piece'] = df.apply(lambda row: np.concatenate([\n",
    "    row['king_safety'],\n",
    "    row['normalized_pawn_structure']\n",
    "]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fen</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>category</th>\n",
       "      <th>bitstring</th>\n",
       "      <th>king_safety</th>\n",
       "      <th>material</th>\n",
       "      <th>space</th>\n",
       "      <th>pawn_structure</th>\n",
       "      <th>development</th>\n",
       "      <th>normalized_material</th>\n",
       "      <th>...</th>\n",
       "      <th>piece</th>\n",
       "      <th>bitstring_tensor</th>\n",
       "      <th>king_safety_tensor</th>\n",
       "      <th>material_tensor</th>\n",
       "      <th>space_tensor</th>\n",
       "      <th>pawn_structure_tensor</th>\n",
       "      <th>development_tensor</th>\n",
       "      <th>vector_tensor</th>\n",
       "      <th>global_tensor</th>\n",
       "      <th>piece_tensor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...</td>\n",
       "      <td>5.2</td>\n",
       "      <td>White is winning</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[5.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...</td>\n",
       "      <td>[1, 5]</td>\n",
       "      <td>[2, 0, 0, 2, 4, 2, 0, 0, 1, 4]</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]</td>\n",
       "      <td>[1.0, 0.0, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>[tensor(0.), tensor(1.), tensor(0.), tensor(0....</td>\n",
       "      <td>[tensor(1.), tensor(0.), tensor(0.2000), tenso...</td>\n",
       "      <td>[tensor(0.), tensor(1.)]</td>\n",
       "      <td>[tensor(0.5000), tensor(0.), tensor(0.), tenso...</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(0....</td>\n",
       "      <td>[tensor(0.), tensor(1.), tensor(0.), tensor(0....</td>\n",
       "      <td>[tensor(5.), tensor(0.), tensor(1.), tensor(1....</td>\n",
       "      <td>[tensor(0.), tensor(1.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...</td>\n",
       "      <td>5.2</td>\n",
       "      <td>White is winning</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[5.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...</td>\n",
       "      <td>[1, 5]</td>\n",
       "      <td>[2, 0, 0, 2, 4, 2, 0, 0, 1, 4]</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]</td>\n",
       "      <td>[1.0, 0.0, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>[tensor(0.), tensor(1.), tensor(0.), tensor(0....</td>\n",
       "      <td>[tensor(1.), tensor(0.), tensor(0.2000), tenso...</td>\n",
       "      <td>[tensor(0.), tensor(1.)]</td>\n",
       "      <td>[tensor(0.5000), tensor(0.), tensor(0.), tenso...</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(0....</td>\n",
       "      <td>[tensor(0.), tensor(1.), tensor(0.), tensor(0....</td>\n",
       "      <td>[tensor(5.), tensor(0.), tensor(1.), tensor(1....</td>\n",
       "      <td>[tensor(0.), tensor(1.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Even</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...</td>\n",
       "      <td>[31, 0]</td>\n",
       "      <td>[2, 0, 0, 1, 3, 0, 0, 0, 1, 2]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 1, 1, 2, 1]</td>\n",
       "      <td>[1.0, 0.0, 0.25, 0.25, 0.25, 0.25, 1.0, 0.25, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(1.), tensor(0....</td>\n",
       "      <td>[tensor(1.), tensor(0.), tensor(0.2500), tenso...</td>\n",
       "      <td>[tensor(1.), tensor(0.)]</td>\n",
       "      <td>[tensor(0.6667), tensor(0.), tensor(0.), tenso...</td>\n",
       "      <td>[tensor(1.), tensor(0.), tensor(1.), tensor(1....</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(1.), tensor(0....</td>\n",
       "      <td>[tensor(4.), tensor(0.), tensor(1.), tensor(1....</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(1.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Even</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...</td>\n",
       "      <td>[31, 0]</td>\n",
       "      <td>[2, 0, 0, 1, 3, 0, 0, 0, 1, 2]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 1, 1, 2, 1]</td>\n",
       "      <td>[1.0, 0.0, 0.25, 0.25, 0.25, 0.25, 1.0, 0.25, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(1.), tensor(0....</td>\n",
       "      <td>[tensor(1.), tensor(0.), tensor(0.2500), tenso...</td>\n",
       "      <td>[tensor(1.), tensor(0.)]</td>\n",
       "      <td>[tensor(0.6667), tensor(0.), tensor(0.), tenso...</td>\n",
       "      <td>[tensor(1.), tensor(0.), tensor(1.), tensor(1....</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(1.), tensor(0....</td>\n",
       "      <td>[tensor(4.), tensor(0.), tensor(1.), tensor(1....</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(1.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2k4r/ppp2p2/2b2B2/7p/6pP/2P4P/PP3Nq1/R5K1 w - ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Even</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[5.0, 1.0, 1.0, 1.0, 0.0, 1.0, 6.0, 0.0, 1.0, ...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[1, 1, 0, 0, 2, 0, 0, 0, 2, 3]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 1, 0, 0]</td>\n",
       "      <td>[0.8333334, 0.16666667, 0.16666667, 0.16666667...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(1....</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>[tensor(0.8333), tensor(0.1667), tensor(0.1667...</td>\n",
       "      <td>[tensor(0.), tensor(1.)]</td>\n",
       "      <td>[tensor(0.3333), tensor(0.3333), tensor(0.), t...</td>\n",
       "      <td>[tensor(1.), tensor(0.), tensor(1.), tensor(1....</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>[tensor(5.), tensor(1.), tensor(1.), tensor(1....</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 fen  evaluation  \\\n",
       "0  r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...         5.2   \n",
       "1  r3k3/p4n1p/3p1Bq1/3P4/2p2P2/7P/P3Q1P1/5R1K b q...         5.2   \n",
       "2  4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...         0.0   \n",
       "3  4Q3/7k/2p4p/1p1qP1p1/3B1nR1/2P4P/3r2P1/6K1 w -...         0.0   \n",
       "4  2k4r/ppp2p2/2b2B2/7p/6pP/2P4P/PP3Nq1/R5K1 w - ...         0.0   \n",
       "\n",
       "           category                                          bitstring  \\\n",
       "0  White is winning  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  White is winning  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2              Even  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3              Even  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4              Even  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                         king_safety  \\\n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...   \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                            material    space  \\\n",
       "0  [5.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...   [1, 5]   \n",
       "1  [5.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...   [1, 5]   \n",
       "2  [4.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...  [31, 0]   \n",
       "3  [4.0, 0.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 0.0, ...  [31, 0]   \n",
       "4  [5.0, 1.0, 1.0, 1.0, 0.0, 1.0, 6.0, 0.0, 1.0, ...   [0, 1]   \n",
       "\n",
       "                   pawn_structure                     development  \\\n",
       "0  [2, 0, 0, 2, 4, 2, 0, 0, 1, 4]  [1, 1, 1, 0, 1, 1, 1, 1, 1, 0]   \n",
       "1  [2, 0, 0, 2, 4, 2, 0, 0, 1, 4]  [1, 1, 1, 0, 1, 1, 1, 1, 1, 0]   \n",
       "2  [2, 0, 0, 1, 3, 0, 0, 0, 1, 2]  [1, 0, 1, 1, 1, 1, 1, 1, 2, 1]   \n",
       "3  [2, 0, 0, 1, 3, 0, 0, 0, 1, 2]  [1, 0, 1, 1, 1, 1, 1, 1, 2, 1]   \n",
       "4  [1, 1, 0, 0, 2, 0, 0, 0, 2, 3]  [1, 0, 1, 1, 1, 1, 0, 1, 0, 0]   \n",
       "\n",
       "                                 normalized_material  ...  \\\n",
       "0  [1.0, 0.0, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.0, ...  ...   \n",
       "1  [1.0, 0.0, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.0, ...  ...   \n",
       "2  [1.0, 0.0, 0.25, 0.25, 0.25, 0.25, 1.0, 0.25, ...  ...   \n",
       "3  [1.0, 0.0, 0.25, 0.25, 0.25, 0.25, 1.0, 0.25, ...  ...   \n",
       "4  [0.8333334, 0.16666667, 0.16666667, 0.16666667...  ...   \n",
       "\n",
       "                                               piece  \\\n",
       "0  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n",
       "1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n",
       "2  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n",
       "3  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n",
       "\n",
       "                                    bitstring_tensor  \\\n",
       "0  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "1  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "2  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "3  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "4  [tensor(0.), tensor(0.), tensor(0.), tensor(1....   \n",
       "\n",
       "                                  king_safety_tensor  \\\n",
       "0  [tensor(0.), tensor(1.), tensor(0.), tensor(0....   \n",
       "1  [tensor(0.), tensor(1.), tensor(0.), tensor(0....   \n",
       "2  [tensor(0.), tensor(0.), tensor(1.), tensor(0....   \n",
       "3  [tensor(0.), tensor(0.), tensor(1.), tensor(0....   \n",
       "4  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "\n",
       "                                     material_tensor  \\\n",
       "0  [tensor(1.), tensor(0.), tensor(0.2000), tenso...   \n",
       "1  [tensor(1.), tensor(0.), tensor(0.2000), tenso...   \n",
       "2  [tensor(1.), tensor(0.), tensor(0.2500), tenso...   \n",
       "3  [tensor(1.), tensor(0.), tensor(0.2500), tenso...   \n",
       "4  [tensor(0.8333), tensor(0.1667), tensor(0.1667...   \n",
       "\n",
       "               space_tensor  \\\n",
       "0  [tensor(0.), tensor(1.)]   \n",
       "1  [tensor(0.), tensor(1.)]   \n",
       "2  [tensor(1.), tensor(0.)]   \n",
       "3  [tensor(1.), tensor(0.)]   \n",
       "4  [tensor(0.), tensor(1.)]   \n",
       "\n",
       "                               pawn_structure_tensor  \\\n",
       "0  [tensor(0.5000), tensor(0.), tensor(0.), tenso...   \n",
       "1  [tensor(0.5000), tensor(0.), tensor(0.), tenso...   \n",
       "2  [tensor(0.6667), tensor(0.), tensor(0.), tenso...   \n",
       "3  [tensor(0.6667), tensor(0.), tensor(0.), tenso...   \n",
       "4  [tensor(0.3333), tensor(0.3333), tensor(0.), t...   \n",
       "\n",
       "                                  development_tensor  \\\n",
       "0  [tensor(1.), tensor(1.), tensor(1.), tensor(0....   \n",
       "1  [tensor(1.), tensor(1.), tensor(1.), tensor(0....   \n",
       "2  [tensor(1.), tensor(0.), tensor(1.), tensor(1....   \n",
       "3  [tensor(1.), tensor(0.), tensor(1.), tensor(1....   \n",
       "4  [tensor(1.), tensor(0.), tensor(1.), tensor(1....   \n",
       "\n",
       "                                       vector_tensor  \\\n",
       "0  [tensor(0.), tensor(1.), tensor(0.), tensor(0....   \n",
       "1  [tensor(0.), tensor(1.), tensor(0.), tensor(0....   \n",
       "2  [tensor(0.), tensor(0.), tensor(1.), tensor(0....   \n",
       "3  [tensor(0.), tensor(0.), tensor(1.), tensor(0....   \n",
       "4  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "\n",
       "                                       global_tensor  \\\n",
       "0  [tensor(5.), tensor(0.), tensor(1.), tensor(1....   \n",
       "1  [tensor(5.), tensor(0.), tensor(1.), tensor(1....   \n",
       "2  [tensor(4.), tensor(0.), tensor(1.), tensor(1....   \n",
       "3  [tensor(4.), tensor(0.), tensor(1.), tensor(1....   \n",
       "4  [tensor(5.), tensor(1.), tensor(1.), tensor(1....   \n",
       "\n",
       "                                        piece_tensor  \n",
       "0  [tensor(0.), tensor(1.), tensor(0.), tensor(0....  \n",
       "1  [tensor(0.), tensor(1.), tensor(0.), tensor(0....  \n",
       "2  [tensor(0.), tensor(0.), tensor(1.), tensor(0....  \n",
       "3  [tensor(0.), tensor(0.), tensor(1.), tensor(0....  \n",
       "4  [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bitstring_tensor'] = df['bitstring'].apply(lambda x: torch.tensor([bit for bit in x], dtype=torch.float32))\n",
    "df['king_safety_tensor'] = df['king_safety'].apply(lambda x: torch.tensor([bit for bit in x], dtype=torch.float32))\n",
    "df['material_tensor'] = df['normalized_material'].apply(lambda x: torch.tensor([bit for bit in x], dtype=torch.float32))\n",
    "df['space_tensor'] = df['normalized_space'].apply(lambda x: torch.tensor([bit for bit in x], dtype=torch.float32))\n",
    "df['pawn_structure_tensor'] = df['normalized_pawn_structure'].apply(lambda x: torch.tensor([bit for bit in x], dtype=torch.float32))\n",
    "df['development_tensor'] = df['development'].apply(lambda x: torch.tensor([bit for bit in x], dtype=torch.float32))\n",
    "df['vector_tensor'] = df['vector'].apply(lambda x: torch.tensor([bit for bit in x], dtype=torch.float32))\n",
    "df['global_tensor'] = df['global'].apply(lambda x: torch.tensor([bit for bit in x], dtype=torch.float32))\n",
    "df['piece_tensor'] = df['piece'].apply(lambda x: torch.tensor([bit for bit in x], dtype=torch.float32))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([773])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([18])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([12])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([2])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([10])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([10])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([52])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([24])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([28])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(df['bitstring_tensor'][0].shape)\n",
    "print(type(df['bitstring_tensor'][0]))\n",
    "print(df['king_safety_tensor'][0].shape)\n",
    "print(type(df['king_safety_tensor'][0]))\n",
    "print(df['material_tensor'][0].shape)\n",
    "print(type(df['material_tensor'][0]))\n",
    "print(df['space_tensor'][0].shape)\n",
    "print(type(df['space_tensor'][0]))\n",
    "print(df['pawn_structure_tensor'][0].shape)\n",
    "print(type(df['pawn_structure_tensor'][0]))\n",
    "print(df['development_tensor'][0].shape)\n",
    "print(type(df['development_tensor'][0]))\n",
    "print(df['vector_tensor'][0].shape)\n",
    "print(type(df['vector_tensor'][0]))\n",
    "print(df['global_tensor'][0].shape)\n",
    "print(type(df['global_tensor'][0]))\n",
    "print(df['piece_tensor'][0].shape)\n",
    "print(type(df['piece_tensor'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53841 11537 11538\n"
     ]
    }
   ],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(len(train_df), len(valid_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalDeepChessDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.df.iloc[idx]['king_safety_tensor']\n",
    "        x2 = self.df.iloc[idx]['material_tensor']\n",
    "        x3 = self.df.iloc[idx]['space_tensor']\n",
    "        x4 = self.df.iloc[idx]['pawn_structure_tensor']\n",
    "        x5 = self.df.iloc[idx]['development_tensor']\n",
    "        label = self.df.iloc[idx]['evaluation']\n",
    "\n",
    "        return x1, x2, x3, x4, x5, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53841 11537 11538\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MultiModalDeepChessDataset(train_df)\n",
    "valid_dataset = MultiModalDeepChessDataset(valid_df)\n",
    "test_dataset = MultiModalDeepChessDataset(test_df)\n",
    "\n",
    "print(len(train_dataset), len(valid_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMDBN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MMDBN, self).__init__()\n",
    "        self.mmdbn = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mmdbn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalDeepChess(nn.Module):\n",
    "    def __init__(self, input_size_1, input_size_2, input_size_3, input_size_4, input_size_5):\n",
    "        super(MultiModalDeepChess, self).__init__()\n",
    "        self.branch_1 = MMDBN(input_size_1)\n",
    "        self.branch_2 = MMDBN(input_size_2)\n",
    "        self.branch_3 = MMDBN(input_size_3)\n",
    "        self.branch_4 = MMDBN(input_size_4)\n",
    "        self.branch_5 = MMDBN(input_size_5)\n",
    "\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Linear(1000, 800),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(800, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4, x5):\n",
    "        x1 = self.branch_1(x1)\n",
    "        x2 = self.branch_2(x2)\n",
    "        x3 = self.branch_3(x3)\n",
    "        x4 = self.branch_4(x4)\n",
    "        x5 = self.branch_5(x5)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "\n",
    "        return self.merge_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_loss(file_path=\"../output/best_loss.txt\"):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read().strip()\n",
    "            if content == \"\":\n",
    "                return float(\"inf\")\n",
    "            return float(content)\n",
    "        \n",
    "    return float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, criterion, optimizer, epochs):\n",
    "    best_valid_loss = load_best_loss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for x1, x2, x3, x4, x5, labels in train_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        valid_loss = validate(model, valid_loader, criterion)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Valid Loss: {valid_loss:.4f}')\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"../output/model.pt\")\n",
    "\n",
    "            with open(\"../output/best_loss.txt\", \"w\") as f:\n",
    "                f.write(str(best_valid_loss))\n",
    "\n",
    "            print(f'Model saved with Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "def validate(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, x5, labels in valid_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    return valid_loss / len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, x5, labels in test_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 6.4559, Valid Loss: 6.1592\n",
      "Epoch [2/50], Train Loss: 5.7910, Valid Loss: 5.8136\n",
      "Epoch [3/50], Train Loss: 5.6363, Valid Loss: 5.8687\n",
      "Epoch [4/50], Train Loss: 5.5277, Valid Loss: 5.7221\n",
      "Epoch [5/50], Train Loss: 5.4262, Valid Loss: 5.5181\n",
      "Epoch [6/50], Train Loss: 5.3534, Valid Loss: 5.4746\n",
      "Epoch [7/50], Train Loss: 5.2542, Valid Loss: 5.6617\n",
      "Epoch [8/50], Train Loss: 5.2081, Valid Loss: 5.5015\n",
      "Epoch [9/50], Train Loss: 5.1241, Valid Loss: 5.5416\n",
      "Epoch [10/50], Train Loss: 5.0415, Valid Loss: 5.4364\n",
      "Epoch [11/50], Train Loss: 4.9533, Valid Loss: 5.3911\n",
      "Epoch [12/50], Train Loss: 4.8820, Valid Loss: 5.4782\n",
      "Epoch [13/50], Train Loss: 4.7889, Valid Loss: 5.2286\n",
      "Epoch [14/50], Train Loss: 4.7178, Valid Loss: 5.1245\n",
      "Epoch [15/50], Train Loss: 4.5988, Valid Loss: 5.1988\n",
      "Epoch [16/50], Train Loss: 4.5140, Valid Loss: 5.2154\n",
      "Epoch [17/50], Train Loss: 4.4129, Valid Loss: 5.0447\n",
      "Epoch [18/50], Train Loss: 4.3258, Valid Loss: 4.9405\n",
      "Epoch [19/50], Train Loss: 4.2098, Valid Loss: 5.1406\n",
      "Epoch [20/50], Train Loss: 4.0989, Valid Loss: 4.9104\n",
      "Epoch [21/50], Train Loss: 4.0021, Valid Loss: 4.8804\n",
      "Epoch [22/50], Train Loss: 3.9143, Valid Loss: 4.8661\n",
      "Epoch [23/50], Train Loss: 3.7726, Valid Loss: 4.7780\n",
      "Epoch [24/50], Train Loss: 3.6755, Valid Loss: 4.8084\n",
      "Epoch [25/50], Train Loss: 3.5537, Valid Loss: 4.7337\n",
      "Epoch [26/50], Train Loss: 3.4086, Valid Loss: 4.5639\n",
      "Epoch [27/50], Train Loss: 3.3327, Valid Loss: 4.5874\n",
      "Epoch [28/50], Train Loss: 3.1695, Valid Loss: 4.5194\n",
      "Epoch [29/50], Train Loss: 3.0936, Valid Loss: 4.4580\n",
      "Epoch [30/50], Train Loss: 2.9783, Valid Loss: 4.2883\n",
      "Epoch [31/50], Train Loss: 2.8390, Valid Loss: 4.2801\n",
      "Epoch [32/50], Train Loss: 2.7877, Valid Loss: 4.2804\n",
      "Epoch [33/50], Train Loss: 2.6652, Valid Loss: 4.0862\n",
      "Epoch [34/50], Train Loss: 2.5893, Valid Loss: 4.2086\n",
      "Epoch [35/50], Train Loss: 2.4734, Valid Loss: 4.3084\n",
      "Epoch [36/50], Train Loss: 2.3602, Valid Loss: 4.0637\n",
      "Epoch [37/50], Train Loss: 2.3495, Valid Loss: 3.9335\n",
      "Epoch [38/50], Train Loss: 2.2470, Valid Loss: 3.8811\n",
      "Epoch [39/50], Train Loss: 2.1473, Valid Loss: 3.8664\n",
      "Epoch [40/50], Train Loss: 2.0784, Valid Loss: 3.6695\n",
      "Epoch [41/50], Train Loss: 2.0276, Valid Loss: 3.7477\n",
      "Epoch [42/50], Train Loss: 1.9027, Valid Loss: 3.6786\n",
      "Epoch [43/50], Train Loss: 1.8917, Valid Loss: 3.7354\n",
      "Epoch [44/50], Train Loss: 1.8183, Valid Loss: 3.7490\n",
      "Epoch [45/50], Train Loss: 1.7819, Valid Loss: 3.5487\n",
      "Epoch [46/50], Train Loss: 1.7374, Valid Loss: 3.6500\n",
      "Epoch [47/50], Train Loss: 1.6875, Valid Loss: 3.3461\n",
      "Epoch [48/50], Train Loss: 1.6654, Valid Loss: 3.4450\n",
      "Epoch [49/50], Train Loss: 1.6002, Valid Loss: 3.4895\n",
      "Epoch [50/50], Train Loss: 1.5802, Valid Loss: 3.4401\n",
      "Test Loss: 3.2430\n"
     ]
    }
   ],
   "source": [
    "# not normalized\n",
    "input_size_1 = len(df['king_safety_tensor'][0])\n",
    "input_size_2 = len(df['material_tensor'][0])\n",
    "input_size_3 = len(df['space_tensor'][0])\n",
    "input_size_4 = len(df['pawn_structure_tensor'][0])\n",
    "input_size_5 = len(df['development_tensor'][0])\n",
    "num_epochs = 50\n",
    "model = MultiModalDeepChess(input_size_1, input_size_2, input_size_3, input_size_4, input_size_5).to(device)\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, train_loader, valid_loader, criterion, optimizer, num_epochs)\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 71278161622813.7031, Valid Loss: 10.7316, Learning Rate = [0.1]\n",
      "Epoch [2/50], Train Loss: 10.4516, Valid Loss: 10.7316, Learning Rate = [0.1]\n",
      "Epoch [3/50], Train Loss: 10.4516, Valid Loss: 10.7316, Learning Rate = [0.1]\n",
      "Epoch [4/50], Train Loss: 10.4506, Valid Loss: 10.7316, Learning Rate = [0.1]\n",
      "Epoch [5/50], Train Loss: 10.4511, Valid Loss: 10.7316, Learning Rate = [0.1]\n",
      "Epoch [6/50], Train Loss: 10.4496, Valid Loss: 10.7316, Learning Rate = [0.1]\n",
      "Epoch [7/50], Train Loss: 10.4511, Valid Loss: 10.7315, Learning Rate = [0.1]\n",
      "Epoch [8/50], Train Loss: 10.4505, Valid Loss: 10.7314, Learning Rate = [0.1]\n",
      "Epoch [9/50], Train Loss: 10.4501, Valid Loss: 10.7310, Learning Rate = [0.1]\n",
      "Epoch [10/50], Train Loss: 10.4509, Valid Loss: 10.7302, Learning Rate = [0.010000000000000002]\n",
      "Epoch [11/50], Train Loss: 10.4495, Valid Loss: 10.7300, Learning Rate = [0.010000000000000002]\n",
      "Epoch [12/50], Train Loss: 10.4497, Valid Loss: 10.7296, Learning Rate = [0.010000000000000002]\n",
      "Epoch [13/50], Train Loss: 10.4500, Valid Loss: 10.7287, Learning Rate = [0.010000000000000002]\n",
      "Epoch [14/50], Train Loss: 10.4464, Valid Loss: 10.7272, Learning Rate = [0.010000000000000002]\n",
      "Epoch [15/50], Train Loss: 10.4450, Valid Loss: 10.7248, Learning Rate = [0.010000000000000002]\n",
      "Epoch [16/50], Train Loss: 10.4450, Valid Loss: 10.7231, Learning Rate = [0.010000000000000002]\n",
      "Epoch [17/50], Train Loss: 10.4428, Valid Loss: 10.7223, Learning Rate = [0.010000000000000002]\n",
      "Epoch [18/50], Train Loss: 10.4425, Valid Loss: 10.7228, Learning Rate = [0.010000000000000002]\n",
      "Epoch [19/50], Train Loss: 10.4437, Valid Loss: 10.7223, Learning Rate = [0.010000000000000002]\n",
      "Epoch [20/50], Train Loss: 10.4459, Valid Loss: 10.7256, Learning Rate = [0.0010000000000000002]\n",
      "Epoch [21/50], Train Loss: 10.4427, Valid Loss: 10.7223, Learning Rate = [0.0010000000000000002]\n",
      "Epoch [22/50], Train Loss: 10.4427, Valid Loss: 10.7223, Learning Rate = [0.0010000000000000002]\n",
      "Epoch [23/50], Train Loss: 10.4410, Valid Loss: 10.7222, Learning Rate = [0.0010000000000000002]\n",
      "Epoch [24/50], Train Loss: 10.4445, Valid Loss: 10.7223, Learning Rate = [0.0010000000000000002]\n",
      "Epoch [25/50], Train Loss: 10.4440, Valid Loss: 10.7222, Learning Rate = [0.0010000000000000002]\n",
      "Epoch [26/50], Train Loss: 10.4428, Valid Loss: 10.7225, Learning Rate = [0.0010000000000000002]\n",
      "Epoch [27/50], Train Loss: 10.4434, Valid Loss: 10.7224, Learning Rate = [0.0010000000000000002]\n",
      "Epoch [28/50], Train Loss: 10.4423, Valid Loss: 10.7223, Learning Rate = [0.0010000000000000002]\n",
      "Epoch [29/50], Train Loss: 10.4422, Valid Loss: 10.7228, Learning Rate = [0.0010000000000000002]\n",
      "Epoch [30/50], Train Loss: 10.4425, Valid Loss: 10.7222, Learning Rate = [0.00010000000000000003]\n",
      "Epoch [31/50], Train Loss: 10.4413, Valid Loss: 10.7222, Learning Rate = [0.00010000000000000003]\n",
      "Epoch [32/50], Train Loss: 10.4446, Valid Loss: 10.7222, Learning Rate = [0.00010000000000000003]\n",
      "Epoch [33/50], Train Loss: 10.4415, Valid Loss: 10.7222, Learning Rate = [0.00010000000000000003]\n",
      "Epoch [34/50], Train Loss: 10.4405, Valid Loss: 10.7222, Learning Rate = [0.00010000000000000003]\n",
      "Epoch [35/50], Train Loss: 10.4418, Valid Loss: 10.7222, Learning Rate = [0.00010000000000000003]\n",
      "Epoch [36/50], Train Loss: 10.4420, Valid Loss: 10.7222, Learning Rate = [0.00010000000000000003]\n",
      "Epoch [37/50], Train Loss: 10.4411, Valid Loss: 10.7222, Learning Rate = [0.00010000000000000003]\n",
      "Epoch [38/50], Train Loss: 10.4419, Valid Loss: 10.7222, Learning Rate = [0.00010000000000000003]\n",
      "Epoch [39/50], Train Loss: 10.4416, Valid Loss: 10.7222, Learning Rate = [0.00010000000000000003]\n",
      "Epoch [40/50], Train Loss: 10.4442, Valid Loss: 10.7222, Learning Rate = [1.0000000000000004e-05]\n",
      "Epoch [41/50], Train Loss: 10.4426, Valid Loss: 10.7222, Learning Rate = [1.0000000000000004e-05]\n",
      "Epoch [42/50], Train Loss: 10.4418, Valid Loss: 10.7222, Learning Rate = [1.0000000000000004e-05]\n",
      "Epoch [43/50], Train Loss: 10.4420, Valid Loss: 10.7222, Learning Rate = [1.0000000000000004e-05]\n",
      "Epoch [44/50], Train Loss: 10.4431, Valid Loss: 10.7222, Learning Rate = [1.0000000000000004e-05]\n",
      "Epoch [45/50], Train Loss: 10.4407, Valid Loss: 10.7222, Learning Rate = [1.0000000000000004e-05]\n",
      "Epoch [46/50], Train Loss: 10.4426, Valid Loss: 10.7222, Learning Rate = [1.0000000000000004e-05]\n",
      "Epoch [47/50], Train Loss: 10.4428, Valid Loss: 10.7222, Learning Rate = [1.0000000000000004e-05]\n",
      "Epoch [48/50], Train Loss: 10.4407, Valid Loss: 10.7222, Learning Rate = [1.0000000000000004e-05]\n",
      "Epoch [49/50], Train Loss: 10.4422, Valid Loss: 10.7222, Learning Rate = [1.0000000000000004e-05]\n",
      "Epoch [50/50], Train Loss: 10.4425, Valid Loss: 10.7222, Learning Rate = [1.0000000000000004e-06]\n",
      "Test Loss: 10.3325\n"
     ]
    }
   ],
   "source": [
    "# Normalized, use stepLR scheduler\n",
    "class MultiModalDeepChessDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.df.iloc[idx]['king_safety_tensor']\n",
    "        x2 = self.df.iloc[idx]['material_tensor']\n",
    "        x3 = self.df.iloc[idx]['space_tensor']\n",
    "        x4 = self.df.iloc[idx]['pawn_structure_tensor']\n",
    "        x5 = self.df.iloc[idx]['development_tensor']\n",
    "        label = self.df.iloc[idx]['evaluation']\n",
    "\n",
    "        return x1, x2, x3, x4, x5, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "train_dataset = MultiModalDeepChessDataset(train_df)\n",
    "valid_dataset = MultiModalDeepChessDataset(valid_df)\n",
    "test_dataset = MultiModalDeepChessDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class MMDBN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MMDBN, self).__init__()\n",
    "        self.mmdbn = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mmdbn(x)\n",
    "    \n",
    "class MultiModalDeepChess(nn.Module):\n",
    "    def __init__(self, input_size_1, input_size_2, input_size_3, input_size_4, input_size_5):\n",
    "        super(MultiModalDeepChess, self).__init__()\n",
    "        self.branch_1 = MMDBN(input_size_1)\n",
    "        self.branch_2 = MMDBN(input_size_2)\n",
    "        self.branch_3 = MMDBN(input_size_3)\n",
    "        self.branch_4 = MMDBN(input_size_4)\n",
    "        self.branch_5 = MMDBN(input_size_5)\n",
    "\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Linear(1000, 800),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(800, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4, x5):\n",
    "        x1 = self.branch_1(x1)\n",
    "        x2 = self.branch_2(x2)\n",
    "        x3 = self.branch_3(x3)\n",
    "        x4 = self.branch_4(x4)\n",
    "        x5 = self.branch_5(x5)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "\n",
    "        return self.merge_layers(x)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_best_loss(file_path=\"../output/best_loss.txt\"):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read().strip()\n",
    "            if content == \"\":\n",
    "                return float(\"inf\")\n",
    "            return float(content)\n",
    "        \n",
    "    return float(\"inf\")\n",
    "\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, epochs, scheduler):\n",
    "    best_valid_loss = load_best_loss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for x1, x2, x3, x4, x5, labels in train_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        scheduler.step() \n",
    "\n",
    "        valid_loss = validate(model, valid_loader, criterion)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Valid Loss: {valid_loss:.4f}, Learning Rate = {scheduler.get_last_lr()}')\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"../output/model.pt\")\n",
    "\n",
    "            with open(\"../output/best_loss.txt\", \"w\") as f:\n",
    "                f.write(str(best_valid_loss))\n",
    "\n",
    "            print(f'Model saved with Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "def validate(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, x5, labels in valid_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    return valid_loss / len(valid_loader)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, x5, labels in test_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}')\n",
    "\n",
    "input_size_1 = len(df['king_safety_tensor'][0])\n",
    "input_size_2 = len(df['material_tensor'][0])\n",
    "input_size_3 = len(df['space_tensor'][0])\n",
    "input_size_4 = len(df['pawn_structure_tensor'][0])\n",
    "input_size_5 = len(df['development_tensor'][0])\n",
    "num_epochs = 50\n",
    "model = MultiModalDeepChess(input_size_1, input_size_2, input_size_3, input_size_4, input_size_5).to(device)\n",
    "learning_rate = 0.1\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "train(model, train_loader, valid_loader, criterion, optimizer, num_epochs, scheduler)\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 15731.3064, Valid Loss: 10.7256, Learning Rate: [0.01]\n",
      "Epoch [2/50], Train Loss: 10.4451, Valid Loss: 10.7243, Learning Rate: [0.01]\n",
      "Epoch [3/50], Train Loss: 10.4422, Valid Loss: 10.7228, Learning Rate: [0.01]\n",
      "Epoch [4/50], Train Loss: 10.4440, Valid Loss: 10.7226, Learning Rate: [0.01]\n",
      "Epoch [5/50], Train Loss: 10.4425, Valid Loss: 10.7223, Learning Rate: [0.01]\n",
      "Epoch [6/50], Train Loss: 10.4420, Valid Loss: 10.7239, Learning Rate: [0.01]\n",
      "Epoch [7/50], Train Loss: 10.4443, Valid Loss: 10.7222, Learning Rate: [0.01]\n",
      "Epoch [8/50], Train Loss: 10.4428, Valid Loss: 10.7243, Learning Rate: [0.01]\n",
      "Epoch [9/50], Train Loss: 10.4436, Valid Loss: 10.7222, Learning Rate: [0.001]\n",
      "Epoch [10/50], Train Loss: 10.4430, Valid Loss: 10.7223, Learning Rate: [0.001]\n",
      "Epoch [11/50], Train Loss: 10.4408, Valid Loss: 10.7223, Learning Rate: [0.001]\n",
      "Epoch [12/50], Train Loss: 10.4424, Valid Loss: 10.7224, Learning Rate: [0.001]\n",
      "Epoch [13/50], Train Loss: 10.4423, Valid Loss: 10.7225, Learning Rate: [0.001]\n",
      "Epoch [14/50], Train Loss: 10.4411, Valid Loss: 10.7222, Learning Rate: [0.001]\n",
      "Epoch [15/50], Train Loss: 10.4414, Valid Loss: 10.7226, Learning Rate: [0.0001]\n",
      "Epoch [16/50], Train Loss: 10.4424, Valid Loss: 10.7225, Learning Rate: [0.0001]\n",
      "Epoch [17/50], Train Loss: 10.4416, Valid Loss: 10.7224, Learning Rate: [0.0001]\n",
      "Epoch [18/50], Train Loss: 10.4422, Valid Loss: 10.7223, Learning Rate: [0.0001]\n",
      "Epoch [19/50], Train Loss: 10.4419, Valid Loss: 10.7223, Learning Rate: [0.0001]\n",
      "Epoch [20/50], Train Loss: 10.4413, Valid Loss: 10.7223, Learning Rate: [0.0001]\n",
      "Epoch [21/50], Train Loss: 10.4402, Valid Loss: 10.7223, Learning Rate: [1e-05]\n",
      "Epoch [22/50], Train Loss: 10.4417, Valid Loss: 10.7223, Learning Rate: [1e-05]\n",
      "Epoch [23/50], Train Loss: 10.4428, Valid Loss: 10.7223, Learning Rate: [1e-05]\n",
      "Epoch [24/50], Train Loss: 10.4404, Valid Loss: 10.7223, Learning Rate: [1e-05]\n",
      "Epoch [25/50], Train Loss: 10.4423, Valid Loss: 10.7223, Learning Rate: [1e-05]\n",
      "Epoch [26/50], Train Loss: 10.4417, Valid Loss: 10.7223, Learning Rate: [1e-05]\n",
      "Epoch [27/50], Train Loss: 10.4408, Valid Loss: 10.7223, Learning Rate: [1.0000000000000002e-06]\n",
      "Epoch [28/50], Train Loss: 10.4412, Valid Loss: 10.7223, Learning Rate: [1.0000000000000002e-06]\n",
      "Epoch [29/50], Train Loss: 10.4417, Valid Loss: 10.7223, Learning Rate: [1.0000000000000002e-06]\n",
      "Epoch [30/50], Train Loss: 10.4427, Valid Loss: 10.7223, Learning Rate: [1.0000000000000002e-06]\n",
      "Epoch [31/50], Train Loss: 10.4398, Valid Loss: 10.7223, Learning Rate: [1.0000000000000002e-06]\n",
      "Epoch [32/50], Train Loss: 10.4416, Valid Loss: 10.7223, Learning Rate: [1.0000000000000002e-06]\n",
      "Epoch [33/50], Train Loss: 10.4431, Valid Loss: 10.7223, Learning Rate: [1.0000000000000002e-07]\n",
      "Epoch [34/50], Train Loss: 10.4418, Valid Loss: 10.7223, Learning Rate: [1.0000000000000002e-07]\n",
      "Epoch [35/50], Train Loss: 10.4431, Valid Loss: 10.7223, Learning Rate: [1.0000000000000002e-07]\n",
      "Epoch [36/50], Train Loss: 10.4424, Valid Loss: 10.7223, Learning Rate: [1.0000000000000002e-07]\n",
      "Epoch [37/50], Train Loss: 10.4415, Valid Loss: 10.7223, Learning Rate: [1.0000000000000002e-07]\n",
      "Epoch [38/50], Train Loss: 10.4421, Valid Loss: 10.7223, Learning Rate: [1.0000000000000002e-07]\n",
      "Epoch [39/50], Train Loss: 10.4415, Valid Loss: 10.7223, Learning Rate: [1.0000000000000004e-08]\n",
      "Epoch [40/50], Train Loss: 10.4424, Valid Loss: 10.7223, Learning Rate: [1.0000000000000004e-08]\n",
      "Epoch [41/50], Train Loss: 10.4415, Valid Loss: 10.7223, Learning Rate: [1.0000000000000004e-08]\n",
      "Epoch [42/50], Train Loss: 10.4411, Valid Loss: 10.7223, Learning Rate: [1.0000000000000004e-08]\n",
      "Epoch [43/50], Train Loss: 10.4415, Valid Loss: 10.7223, Learning Rate: [1.0000000000000004e-08]\n",
      "Epoch [44/50], Train Loss: 10.4422, Valid Loss: 10.7223, Learning Rate: [1.0000000000000004e-08]\n",
      "Epoch [45/50], Train Loss: 10.4417, Valid Loss: 10.7223, Learning Rate: [1.0000000000000004e-08]\n",
      "Epoch [46/50], Train Loss: 10.4424, Valid Loss: 10.7223, Learning Rate: [1.0000000000000004e-08]\n",
      "Epoch [47/50], Train Loss: 10.4420, Valid Loss: 10.7223, Learning Rate: [1.0000000000000004e-08]\n",
      "Epoch [48/50], Train Loss: 10.4422, Valid Loss: 10.7223, Learning Rate: [1.0000000000000004e-08]\n",
      "Epoch [49/50], Train Loss: 10.4430, Valid Loss: 10.7223, Learning Rate: [1.0000000000000004e-08]\n",
      "Epoch [50/50], Train Loss: 10.4427, Valid Loss: 10.7223, Learning Rate: [1.0000000000000004e-08]\n",
      "Test Loss: 10.3326\n"
     ]
    }
   ],
   "source": [
    "# Normalized, use ReduceLROnPlateau scheduler\n",
    "class MultiModalDeepChessDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.df.iloc[idx]['king_safety_tensor']\n",
    "        x2 = self.df.iloc[idx]['material_tensor']\n",
    "        x3 = self.df.iloc[idx]['space_tensor']\n",
    "        x4 = self.df.iloc[idx]['pawn_structure_tensor']\n",
    "        x5 = self.df.iloc[idx]['development_tensor']\n",
    "        label = self.df.iloc[idx]['evaluation']\n",
    "\n",
    "        return x1, x2, x3, x4, x5, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "train_dataset = MultiModalDeepChessDataset(train_df)\n",
    "valid_dataset = MultiModalDeepChessDataset(valid_df)\n",
    "test_dataset = MultiModalDeepChessDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class MMDBN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MMDBN, self).__init__()\n",
    "        self.mmdbn = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mmdbn(x)\n",
    "    \n",
    "class MultiModalDeepChess(nn.Module):\n",
    "    def __init__(self, input_size_1, input_size_2, input_size_3, input_size_4, input_size_5):\n",
    "        super(MultiModalDeepChess, self).__init__()\n",
    "        self.branch_1 = MMDBN(input_size_1)\n",
    "        self.branch_2 = MMDBN(input_size_2)\n",
    "        self.branch_3 = MMDBN(input_size_3)\n",
    "        self.branch_4 = MMDBN(input_size_4)\n",
    "        self.branch_5 = MMDBN(input_size_5)\n",
    "\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Linear(1000, 800),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(800, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4, x5):\n",
    "        x1 = self.branch_1(x1)\n",
    "        x2 = self.branch_2(x2)\n",
    "        x3 = self.branch_3(x3)\n",
    "        x4 = self.branch_4(x4)\n",
    "        x5 = self.branch_5(x5)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "\n",
    "        return self.merge_layers(x)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_best_loss(file_path=\"../output/best_loss.txt\"):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read().strip()\n",
    "            if content == \"\":\n",
    "                return float(\"inf\")\n",
    "            return float(content)\n",
    "        \n",
    "    return float(\"inf\")\n",
    "\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, epochs, scheduler):\n",
    "    best_valid_loss = load_best_loss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for x1, x2, x3, x4, x5, labels in train_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        valid_loss = validate(model, valid_loader, criterion)\n",
    "\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Valid Loss: {valid_loss:.4f}, Learning Rate: {scheduler.get_last_lr()}')\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"../output/model.pt\")\n",
    "\n",
    "            with open(\"../output/best_loss.txt\", \"w\") as f:\n",
    "                f.write(str(best_valid_loss))\n",
    "\n",
    "            print(f'Model saved with Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "def validate(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, x5, labels in valid_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    return valid_loss / len(valid_loader)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, x5, labels in test_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}')\n",
    "\n",
    "input_size_1 = len(df['king_safety_tensor'][0])\n",
    "input_size_2 = len(df['material_tensor'][0])\n",
    "input_size_3 = len(df['space_tensor'][0])\n",
    "input_size_4 = len(df['pawn_structure_tensor'][0])\n",
    "input_size_5 = len(df['development_tensor'][0])\n",
    "num_epochs = 50\n",
    "model = MultiModalDeepChess(input_size_1, input_size_2, input_size_3, input_size_4, input_size_5).to(device)\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1, verbose=True)\n",
    "\n",
    "train(model, train_loader, valid_loader, criterion, optimizer, num_epochs, scheduler)\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 1068959481372932608.0000, Valid Loss: 13.0286, Learning Rate: [0.095]\n",
      "Epoch [2/50], Train Loss: 12.7495, Valid Loss: 13.0286, Learning Rate: [0.09025]\n",
      "Epoch [3/50], Train Loss: 12.7494, Valid Loss: 13.0286, Learning Rate: [0.0857375]\n",
      "Epoch [4/50], Train Loss: 12.7475, Valid Loss: 13.0286, Learning Rate: [0.08145062499999998]\n",
      "Epoch [5/50], Train Loss: 12.7494, Valid Loss: 13.0286, Learning Rate: [0.07737809374999999]\n",
      "Epoch [6/50], Train Loss: 12.7482, Valid Loss: 13.0286, Learning Rate: [0.07350918906249998]\n",
      "Epoch [7/50], Train Loss: 12.7497, Valid Loss: 13.0286, Learning Rate: [0.06983372960937498]\n",
      "Epoch [8/50], Train Loss: 12.7483, Valid Loss: 13.0286, Learning Rate: [0.06634204312890622]\n",
      "Epoch [9/50], Train Loss: 12.7477, Valid Loss: 13.0280, Learning Rate: [0.0630249409724609]\n",
      "Epoch [10/50], Train Loss: 12.7491, Valid Loss: 13.0271, Learning Rate: [0.05987369392383786]\n",
      "Epoch [11/50], Train Loss: 12.7476, Valid Loss: 13.0250, Learning Rate: [0.05688000922764597]\n",
      "Epoch [12/50], Train Loss: 12.7419, Valid Loss: 13.0202, Learning Rate: [0.05403600876626367]\n",
      "Epoch [13/50], Train Loss: 12.7365, Valid Loss: 13.0097, Learning Rate: [0.05133420832795048]\n",
      "Epoch [14/50], Train Loss: 12.7189, Valid Loss: 12.9869, Learning Rate: [0.04876749791155295]\n",
      "Epoch [15/50], Train Loss: 12.6852, Valid Loss: 12.9372, Learning Rate: [0.046329123015975304]\n",
      "Epoch [16/50], Train Loss: 12.6127, Valid Loss: 12.8322, Learning Rate: [0.04401266686517654]\n",
      "Epoch [17/50], Train Loss: 12.4576, Valid Loss: 12.6164, Learning Rate: [0.04181203352191771]\n",
      "Epoch [18/50], Train Loss: 12.1552, Valid Loss: 12.2166, Learning Rate: [0.039721431845821824]\n",
      "Epoch [19/50], Train Loss: 11.6500, Valid Loss: 11.6084, Learning Rate: [0.037735360253530734]\n",
      "Epoch [20/50], Train Loss: 11.0098, Valid Loss: 11.0012, Learning Rate: [0.035848592240854196]\n",
      "Epoch [21/50], Train Loss: 10.5581, Valid Loss: 10.7430, Learning Rate: [0.03405616262881148]\n",
      "Epoch [22/50], Train Loss: 10.4472, Valid Loss: 10.7224, Learning Rate: [0.03235335449737091]\n",
      "Epoch [23/50], Train Loss: 10.4429, Valid Loss: 10.7222, Learning Rate: [0.030735686772502362]\n",
      "Epoch [24/50], Train Loss: 10.4432, Valid Loss: 10.7284, Learning Rate: [0.029198902433877242]\n",
      "Epoch [25/50], Train Loss: 10.4447, Valid Loss: 10.7228, Learning Rate: [0.027738957312183378]\n",
      "Epoch [26/50], Train Loss: 10.4472, Valid Loss: 10.7379, Learning Rate: [0.026352009446574207]\n",
      "Epoch [27/50], Train Loss: 10.4486, Valid Loss: 10.7228, Learning Rate: [0.025034408974245494]\n",
      "Epoch [28/50], Train Loss: 10.4514, Valid Loss: 10.7318, Learning Rate: [0.023782688525533217]\n",
      "Epoch [29/50], Train Loss: 10.4502, Valid Loss: 10.7272, Learning Rate: [0.022593554099256556]\n",
      "Epoch [30/50], Train Loss: 10.4466, Valid Loss: 10.7275, Learning Rate: [0.021463876394293726]\n",
      "Epoch [31/50], Train Loss: 10.4478, Valid Loss: 10.7271, Learning Rate: [0.020390682574579037]\n",
      "Epoch [32/50], Train Loss: 10.4487, Valid Loss: 10.7253, Learning Rate: [0.019371148445850084]\n",
      "Epoch [33/50], Train Loss: 10.4452, Valid Loss: 10.7333, Learning Rate: [0.01840259102355758]\n",
      "Epoch [34/50], Train Loss: 10.4503, Valid Loss: 10.7285, Learning Rate: [0.0174824614723797]\n",
      "Epoch [35/50], Train Loss: 10.4473, Valid Loss: 10.7379, Learning Rate: [0.016608338398760712]\n",
      "Epoch [36/50], Train Loss: 10.4468, Valid Loss: 10.7277, Learning Rate: [0.015777921478822676]\n",
      "Epoch [37/50], Train Loss: 10.4443, Valid Loss: 10.7228, Learning Rate: [0.014989025404881541]\n",
      "Epoch [38/50], Train Loss: 10.4457, Valid Loss: 10.7438, Learning Rate: [0.014239574134637464]\n",
      "Epoch [39/50], Train Loss: 10.4474, Valid Loss: 10.7228, Learning Rate: [0.01352759542790559]\n",
      "Epoch [40/50], Train Loss: 10.4456, Valid Loss: 10.7266, Learning Rate: [0.012851215656510309]\n",
      "Epoch [41/50], Train Loss: 10.4456, Valid Loss: 10.7226, Learning Rate: [0.012208654873684792]\n",
      "Epoch [42/50], Train Loss: 10.4480, Valid Loss: 10.7228, Learning Rate: [0.011598222130000552]\n",
      "Epoch [43/50], Train Loss: 10.4428, Valid Loss: 10.7223, Learning Rate: [0.011018311023500524]\n",
      "Epoch [44/50], Train Loss: 10.4454, Valid Loss: 10.7403, Learning Rate: [0.010467395472325497]\n",
      "Epoch [45/50], Train Loss: 10.4464, Valid Loss: 10.7263, Learning Rate: [0.009944025698709221]\n",
      "Epoch [46/50], Train Loss: 10.4463, Valid Loss: 10.7223, Learning Rate: [0.00944682441377376]\n",
      "Epoch [47/50], Train Loss: 10.4446, Valid Loss: 10.7240, Learning Rate: [0.00897448319308507]\n",
      "Epoch [48/50], Train Loss: 10.4452, Valid Loss: 10.7223, Learning Rate: [0.008525759033430816]\n",
      "Epoch [49/50], Train Loss: 10.4430, Valid Loss: 10.7222, Learning Rate: [0.008099471081759275]\n",
      "Epoch [50/50], Train Loss: 10.4467, Valid Loss: 10.7242, Learning Rate: [0.007694497527671311]\n",
      "Test Loss: 10.3348\n"
     ]
    }
   ],
   "source": [
    "# Normalized, use ExponentialLR scheduler\n",
    "class MultiModalDeepChessDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.df.iloc[idx]['king_safety_tensor']\n",
    "        x2 = self.df.iloc[idx]['material_tensor']\n",
    "        x3 = self.df.iloc[idx]['space_tensor']\n",
    "        x4 = self.df.iloc[idx]['pawn_structure_tensor']\n",
    "        x5 = self.df.iloc[idx]['development_tensor']\n",
    "        label = self.df.iloc[idx]['normalized_evaluation']\n",
    "\n",
    "        return x1, x2, x3, x4, x5, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "train_dataset = MultiModalDeepChessDataset(train_df)\n",
    "valid_dataset = MultiModalDeepChessDataset(valid_df)\n",
    "test_dataset = MultiModalDeepChessDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class MMDBN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MMDBN, self).__init__()\n",
    "        self.mmdbn = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mmdbn(x)\n",
    "    \n",
    "class MultiModalDeepChess(nn.Module):\n",
    "    def __init__(self, input_size_1, input_size_2, input_size_3, input_size_4, input_size_5):\n",
    "        super(MultiModalDeepChess, self).__init__()\n",
    "        self.branch_1 = MMDBN(input_size_1)\n",
    "        self.branch_2 = MMDBN(input_size_2)\n",
    "        self.branch_3 = MMDBN(input_size_3)\n",
    "        self.branch_4 = MMDBN(input_size_4)\n",
    "        self.branch_5 = MMDBN(input_size_5)\n",
    "\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Linear(1000, 800),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(800, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4, x5):\n",
    "        x1 = self.branch_1(x1)\n",
    "        x2 = self.branch_2(x2)\n",
    "        x3 = self.branch_3(x3)\n",
    "        x4 = self.branch_4(x4)\n",
    "        x5 = self.branch_5(x5)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "\n",
    "        return self.merge_layers(x)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_best_loss(file_path=\"../output/best_loss.txt\"):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read().strip()\n",
    "            if content == \"\":\n",
    "                return float(\"inf\")\n",
    "            return float(content)\n",
    "        \n",
    "    return float(\"inf\")\n",
    "\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, epochs, scheduler):\n",
    "    best_valid_loss = load_best_loss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for x1, x2, x3, x4, x5, labels in train_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        valid_loss = validate(model, valid_loader, criterion)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Valid Loss: {valid_loss:.4f}, Learning Rate: {scheduler.get_last_lr()}')\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"../output/model.pt\")\n",
    "\n",
    "            with open(\"../output/best_loss.txt\", \"w\") as f:\n",
    "                f.write(str(best_valid_loss))\n",
    "\n",
    "            print(f'Model saved with Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "def validate(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, x5, labels in valid_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    return valid_loss / len(valid_loader)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, x5, labels in test_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}')\n",
    "\n",
    "input_size_1 = len(df['king_safety_tensor'][0])\n",
    "input_size_2 = len(df['material_tensor'][0])\n",
    "input_size_3 = len(df['space_tensor'][0])\n",
    "input_size_4 = len(df['pawn_structure_tensor'][0])\n",
    "input_size_5 = len(df['development_tensor'][0])\n",
    "num_epochs = 50\n",
    "model = MultiModalDeepChess(input_size_1, input_size_2, input_size_3, input_size_4, input_size_5).to(device)\n",
    "learning_rate = 0.1\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "train(model, train_loader, valid_loader, criterion, optimizer, num_epochs, scheduler)\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 6.5228, Valid Loss: 6.1823, Learning Rate: [0.0009755282581475768]\n",
      "Epoch [2/50], Train Loss: 6.1325, Valid Loss: 6.1672, Learning Rate: [0.0009045084971874736]\n",
      "Epoch [3/50], Train Loss: 6.0235, Valid Loss: 6.0849, Learning Rate: [0.0007938926261462366]\n",
      "Epoch [4/50], Train Loss: 5.8962, Valid Loss: 6.0836, Learning Rate: [0.0006545084971874737]\n",
      "Epoch [5/50], Train Loss: 5.7864, Valid Loss: 6.0141, Learning Rate: [0.0005]\n",
      "Epoch [6/50], Train Loss: 5.6564, Valid Loss: 5.8376, Learning Rate: [0.00034549150281252633]\n",
      "Epoch [7/50], Train Loss: 5.5010, Valid Loss: 5.8596, Learning Rate: [0.0002061073738537635]\n",
      "Epoch [8/50], Train Loss: 5.3361, Valid Loss: 5.7381, Learning Rate: [9.549150281252634e-05]\n",
      "Epoch [9/50], Train Loss: 5.1780, Valid Loss: 5.6647, Learning Rate: [2.4471741852423235e-05]\n",
      "Epoch [10/50], Train Loss: 5.0691, Valid Loss: 5.6534, Learning Rate: [0.0]\n",
      "Epoch [11/50], Train Loss: 5.0295, Valid Loss: 5.6534, Learning Rate: [2.4471741852423235e-05]\n",
      "Epoch [12/50], Train Loss: 5.0392, Valid Loss: 5.6364, Learning Rate: [9.54915028125267e-05]\n",
      "Epoch [13/50], Train Loss: 5.0546, Valid Loss: 5.6326, Learning Rate: [0.0002061073738537643]\n",
      "Epoch [14/50], Train Loss: 5.0723, Valid Loss: 5.6173, Learning Rate: [0.0003454915028125278]\n",
      "Epoch [15/50], Train Loss: 5.1024, Valid Loss: 5.6586, Learning Rate: [0.0005000000000000022]\n",
      "Epoch [16/50], Train Loss: 5.1269, Valid Loss: 5.6410, Learning Rate: [0.0006545084971874766]\n",
      "Epoch [17/50], Train Loss: 5.1928, Valid Loss: 5.6332, Learning Rate: [0.00079389262614624]\n",
      "Epoch [18/50], Train Loss: 5.2041, Valid Loss: 5.7659, Learning Rate: [0.0009045084971874779]\n",
      "Epoch [19/50], Train Loss: 5.2181, Valid Loss: 5.8052, Learning Rate: [0.0009755282581475812]\n",
      "Epoch [20/50], Train Loss: 5.1452, Valid Loss: 5.6805, Learning Rate: [0.0010000000000000046]\n",
      "Epoch [21/50], Train Loss: 5.0603, Valid Loss: 5.6985, Learning Rate: [0.0009755282581475812]\n",
      "Epoch [22/50], Train Loss: 4.9289, Valid Loss: 5.5519, Learning Rate: [0.0009045084971874781]\n",
      "Epoch [23/50], Train Loss: 4.7106, Valid Loss: 5.4079, Learning Rate: [0.0007938926261462403]\n",
      "Epoch [24/50], Train Loss: 4.4594, Valid Loss: 5.3086, Learning Rate: [0.0006545084971874768]\n",
      "Epoch [25/50], Train Loss: 4.1110, Valid Loss: 5.0860, Learning Rate: [0.0005000000000000024]\n",
      "Epoch [26/50], Train Loss: 3.7007, Valid Loss: 4.7514, Learning Rate: [0.00034549150281252807]\n",
      "Epoch [27/50], Train Loss: 3.2679, Valid Loss: 4.5229, Learning Rate: [0.00020610737385376454]\n",
      "Epoch [28/50], Train Loss: 2.8276, Valid Loss: 4.2043, Learning Rate: [9.549150281252684e-05]\n",
      "Epoch [29/50], Train Loss: 2.4719, Valid Loss: 4.0774, Learning Rate: [2.4471741852423408e-05]\n",
      "Epoch [30/50], Train Loss: 2.2560, Valid Loss: 4.0206, Learning Rate: [0.0]\n",
      "Epoch [31/50], Train Loss: 2.1808, Valid Loss: 4.0206, Learning Rate: [2.4471741852423235e-05]\n",
      "Epoch [32/50], Train Loss: 2.1964, Valid Loss: 4.0086, Learning Rate: [9.549150281252638e-05]\n",
      "Epoch [33/50], Train Loss: 2.2228, Valid Loss: 3.9209, Learning Rate: [0.00020610737385376373]\n",
      "Epoch [34/50], Train Loss: 2.2711, Valid Loss: 3.9106, Learning Rate: [0.0003454915028125268]\n",
      "Epoch [35/50], Train Loss: 2.3840, Valid Loss: 3.9265, Learning Rate: [0.0005000000000000009]\n",
      "Epoch [36/50], Train Loss: 2.5633, Valid Loss: 4.0969, Learning Rate: [0.0006545084971874749]\n",
      "Epoch [37/50], Train Loss: 2.7948, Valid Loss: 4.1924, Learning Rate: [0.0007938926261462382]\n",
      "Epoch [38/50], Train Loss: 3.0152, Valid Loss: 4.4749, Learning Rate: [0.0009045084971874756]\n",
      "Epoch [39/50], Train Loss: 3.0611, Valid Loss: 4.3750, Learning Rate: [0.000975528258147579]\n",
      "Epoch [40/50], Train Loss: 3.1541, Valid Loss: 4.9390, Learning Rate: [0.0010000000000000024]\n",
      "Epoch [41/50], Train Loss: 3.0712, Valid Loss: 4.3757, Learning Rate: [0.0009755282581475795]\n",
      "Epoch [42/50], Train Loss: 2.8741, Valid Loss: 4.5009, Learning Rate: [0.0009045084971874761]\n",
      "Epoch [43/50], Train Loss: 2.6090, Valid Loss: 4.0043, Learning Rate: [0.0007938926261462387]\n",
      "Epoch [44/50], Train Loss: 2.3160, Valid Loss: 3.7418, Learning Rate: [0.0006545084971874765]\n",
      "Epoch [45/50], Train Loss: 1.9292, Valid Loss: 3.6357, Learning Rate: [0.0005000000000000014]\n",
      "Epoch [46/50], Train Loss: 1.6011, Valid Loss: 3.2977, Learning Rate: [0.00034549150281252736]\n",
      "Epoch [47/50], Train Loss: 1.3005, Valid Loss: 3.1129, Learning Rate: [0.00020610737385376343]\n",
      "Epoch [48/50], Train Loss: 1.0639, Valid Loss: 2.9462, Learning Rate: [9.549150281252668e-05]\n",
      "Epoch [49/50], Train Loss: 0.9037, Valid Loss: 2.8182, Learning Rate: [2.447174185242363e-05]\n",
      "Epoch [50/50], Train Loss: 0.8071, Valid Loss: 2.8044, Learning Rate: [0.0]\n",
      "Test Loss: 2.6445\n"
     ]
    }
   ],
   "source": [
    "# Normalized, use CosineAnnealingLR scheduler\n",
    "class MultiModalDeepChessDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.df.iloc[idx]['king_safety_tensor']\n",
    "        x2 = self.df.iloc[idx]['material_tensor']\n",
    "        x3 = self.df.iloc[idx]['space_tensor']\n",
    "        x4 = self.df.iloc[idx]['pawn_structure_tensor']\n",
    "        x5 = self.df.iloc[idx]['development_tensor']\n",
    "        label = self.df.iloc[idx]['evaluation']\n",
    "\n",
    "        return x1, x2, x3, x4, x5, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "train_dataset = MultiModalDeepChessDataset(train_df)\n",
    "valid_dataset = MultiModalDeepChessDataset(valid_df)\n",
    "test_dataset = MultiModalDeepChessDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class MMDBN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MMDBN, self).__init__()\n",
    "        self.mmdbn = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mmdbn(x)\n",
    "    \n",
    "class MultiModalDeepChess(nn.Module):\n",
    "    def __init__(self, input_size_1, input_size_2, input_size_3, input_size_4, input_size_5):\n",
    "        super(MultiModalDeepChess, self).__init__()\n",
    "        self.branch_1 = MMDBN(input_size_1)\n",
    "        self.branch_2 = MMDBN(input_size_2)\n",
    "        self.branch_3 = MMDBN(input_size_3)\n",
    "        self.branch_4 = MMDBN(input_size_4)\n",
    "        self.branch_5 = MMDBN(input_size_5)\n",
    "\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Linear(1000, 800),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(800, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4, x5):\n",
    "        x1 = self.branch_1(x1)\n",
    "        x2 = self.branch_2(x2)\n",
    "        x3 = self.branch_3(x3)\n",
    "        x4 = self.branch_4(x4)\n",
    "        x5 = self.branch_5(x5)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "\n",
    "        return self.merge_layers(x)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_best_loss(file_path=\"../output/best_loss.txt\"):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read().strip()\n",
    "            if content == \"\":\n",
    "                return float(\"inf\")\n",
    "            return float(content)\n",
    "        \n",
    "    return float(\"inf\")\n",
    "\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, epochs, scheduler):\n",
    "    best_valid_loss = load_best_loss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for x1, x2, x3, x4, x5, labels in train_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        valid_loss = validate(model, valid_loader, criterion)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Valid Loss: {valid_loss:.4f}, Learning Rate: {scheduler.get_last_lr()}')\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"../output/model.pt\")\n",
    "\n",
    "            with open(\"../output/best_loss.txt\", \"w\") as f:\n",
    "                f.write(str(best_valid_loss))\n",
    "\n",
    "            print(f'Model saved with Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "def validate(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, x5, labels in valid_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    return valid_loss / len(valid_loader)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, x5, labels in test_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}')\n",
    "\n",
    "input_size_1 = len(df['king_safety_tensor'][0])\n",
    "input_size_2 = len(df['material_tensor'][0])\n",
    "input_size_3 = len(df['space_tensor'][0])\n",
    "input_size_4 = len(df['pawn_structure_tensor'][0])\n",
    "input_size_5 = len(df['development_tensor'][0])\n",
    "num_epochs = 50\n",
    "model = MultiModalDeepChess(input_size_1, input_size_2, input_size_3, input_size_4, input_size_5).to(device)\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "train(model, train_loader, valid_loader, criterion, optimizer, num_epochs, scheduler)\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 6.4034, Valid Loss: 6.5264\n",
      "Epoch [2/50], Train Loss: 6.0958, Valid Loss: 6.1455\n",
      "Epoch [3/50], Train Loss: 5.9146, Valid Loss: 6.0207\n",
      "Epoch [4/50], Train Loss: 5.7576, Valid Loss: 5.9600\n",
      "Epoch [5/50], Train Loss: 5.5548, Valid Loss: 5.8275\n",
      "Epoch [6/50], Train Loss: 5.3374, Valid Loss: 5.7582\n",
      "Epoch [7/50], Train Loss: 5.0628, Valid Loss: 5.6193\n",
      "Epoch [8/50], Train Loss: 4.8059, Valid Loss: 5.4697\n",
      "Epoch [9/50], Train Loss: 4.5155, Valid Loss: 5.2217\n",
      "Epoch [10/50], Train Loss: 4.2063, Valid Loss: 5.1304\n",
      "Epoch [11/50], Train Loss: 3.9127, Valid Loss: 4.8872\n",
      "Epoch [12/50], Train Loss: 3.6135, Valid Loss: 4.8339\n",
      "Epoch [13/50], Train Loss: 3.3804, Valid Loss: 4.5841\n",
      "Epoch [14/50], Train Loss: 3.1710, Valid Loss: 4.4734\n",
      "Epoch [15/50], Train Loss: 2.9804, Valid Loss: 4.4227\n",
      "Epoch [16/50], Train Loss: 2.8154, Valid Loss: 4.2667\n",
      "Epoch [17/50], Train Loss: 2.6819, Valid Loss: 4.1256\n",
      "Epoch [18/50], Train Loss: 2.5322, Valid Loss: 4.0443\n",
      "Epoch [19/50], Train Loss: 2.4117, Valid Loss: 3.9046\n",
      "Epoch [20/50], Train Loss: 2.3077, Valid Loss: 4.0963\n",
      "Epoch [21/50], Train Loss: 2.2156, Valid Loss: 3.7945\n",
      "Epoch [22/50], Train Loss: 2.0994, Valid Loss: 3.9029\n",
      "Epoch [23/50], Train Loss: 2.0436, Valid Loss: 3.8929\n",
      "Epoch [24/50], Train Loss: 1.9539, Valid Loss: 3.7702\n",
      "Epoch [25/50], Train Loss: 1.8889, Valid Loss: 3.8033\n",
      "Epoch [26/50], Train Loss: 1.8494, Valid Loss: 3.7276\n",
      "Epoch [27/50], Train Loss: 1.7891, Valid Loss: 3.5209\n",
      "Epoch [28/50], Train Loss: 1.7099, Valid Loss: 3.5688\n",
      "Epoch [29/50], Train Loss: 1.6901, Valid Loss: 3.5152\n",
      "Epoch [30/50], Train Loss: 1.6309, Valid Loss: 3.6302\n",
      "Epoch [31/50], Train Loss: 1.5948, Valid Loss: 3.5876\n",
      "Epoch [32/50], Train Loss: 1.5615, Valid Loss: 3.5690\n",
      "Epoch [33/50], Train Loss: 1.5123, Valid Loss: 3.3521\n",
      "Epoch [34/50], Train Loss: 1.4827, Valid Loss: 3.4817\n",
      "Epoch [35/50], Train Loss: 1.4423, Valid Loss: 3.4036\n",
      "Epoch [36/50], Train Loss: 1.4318, Valid Loss: 3.3181\n",
      "Epoch [37/50], Train Loss: 1.3935, Valid Loss: 3.3992\n",
      "Epoch [38/50], Train Loss: 1.3642, Valid Loss: 3.3298\n",
      "Epoch [39/50], Train Loss: 1.3416, Valid Loss: 3.2182\n",
      "Epoch [40/50], Train Loss: 1.2791, Valid Loss: 3.3137\n",
      "Epoch [41/50], Train Loss: 1.2879, Valid Loss: 3.3330\n",
      "Epoch [42/50], Train Loss: 1.2965, Valid Loss: 3.2268\n",
      "Epoch [43/50], Train Loss: 1.2503, Valid Loss: 3.3163\n",
      "Epoch [44/50], Train Loss: 1.2286, Valid Loss: 3.2507\n",
      "Epoch [45/50], Train Loss: 1.2150, Valid Loss: 3.2096\n",
      "Epoch [46/50], Train Loss: 1.1900, Valid Loss: 3.1411\n",
      "Epoch [47/50], Train Loss: 1.1729, Valid Loss: 3.2040\n",
      "Epoch [48/50], Train Loss: 1.1723, Valid Loss: 3.1468\n",
      "Epoch [49/50], Train Loss: 1.1431, Valid Loss: 3.1611\n",
      "Epoch [50/50], Train Loss: 1.1483, Valid Loss: 3.0853\n",
      "Test Loss: 2.9199\n"
     ]
    }
   ],
   "source": [
    "# Combined all the features\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.df.iloc[idx]['vector_tensor']\n",
    "        x2 = self.df.iloc[idx]['vector_tensor']\n",
    "        label = self.df.iloc[idx]['normalized_evaluation']\n",
    "\n",
    "        return x1, x2, torch.tensor(label, dtype=torch.float32)\n",
    "    \n",
    "train_dataset = MultiModalDataset(train_df)\n",
    "valid_dataset = MultiModalDataset(valid_df)\n",
    "test_dataset = MultiModalDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class MMDBN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MMDBN, self).__init__()\n",
    "        self.mmdbn = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.baselinedbn(x)\n",
    "    \n",
    "class MultiModalDeepChess(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MultiModalDeepChess, self).__init__()\n",
    "        self.branch_1 = MMDBN(input_size)\n",
    "        self.branch_2 = MMDBN(input_size)\n",
    "\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.branch_1(x1)\n",
    "        x2 = self.branch_2(x2)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        return self.merge_layers(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_best_loss(file_path=\"../output/best_loss.txt\"):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read().strip()\n",
    "            if content == \"\":\n",
    "                return float(\"inf\")\n",
    "            return float(content)\n",
    "        \n",
    "    return float(\"inf\")\n",
    "\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, epochs):\n",
    "    best_valid_loss = load_best_loss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for x1, x2, labels in train_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x1, x2)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        valid_loss = validate(model, valid_loader, criterion)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Valid Loss: {valid_loss:.4f}')\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"../output/model.pt\")\n",
    "\n",
    "            with open(\"../output/best_loss.txt\", \"w\") as f:\n",
    "                f.write(str(best_valid_loss))\n",
    "\n",
    "            print(f'Model saved with Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "def validate(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, labels in valid_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    return valid_loss / len(valid_loader)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, labels in test_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}')\n",
    "\n",
    "input_size = len(df['vector_tensor'][0])\n",
    "model = MultiModalDeepChess(input_size).to(device)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, train_loader, valid_loader, criterion, optimizer, num_epochs)\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Batch Size: 32\n",
      "Epoch [1/50], Train Loss: 6.5416, Valid Loss: 6.2721\n",
      "Epoch [2/50], Train Loss: 6.1584, Valid Loss: 6.1247\n",
      "Epoch [3/50], Train Loss: 6.0234, Valid Loss: 6.1499\n",
      "Epoch [4/50], Train Loss: 5.9709, Valid Loss: 6.0870\n",
      "Epoch [5/50], Train Loss: 5.8696, Valid Loss: 6.2268\n",
      "Epoch [6/50], Train Loss: 5.7681, Valid Loss: 5.9814\n",
      "Epoch [7/50], Train Loss: 5.6814, Valid Loss: 5.9273\n",
      "Epoch [8/50], Train Loss: 5.6001, Valid Loss: 5.9931\n",
      "Epoch [9/50], Train Loss: 5.5190, Valid Loss: 5.8591\n",
      "Epoch [10/50], Train Loss: 5.4097, Valid Loss: 6.1447\n",
      "Epoch [11/50], Train Loss: 5.3307, Valid Loss: 5.8444\n",
      "Epoch [12/50], Train Loss: 5.2333, Valid Loss: 5.7480\n",
      "Epoch [13/50], Train Loss: 5.1310, Valid Loss: 5.8376\n",
      "Epoch [14/50], Train Loss: 5.0256, Valid Loss: 5.6813\n",
      "Epoch [15/50], Train Loss: 4.9107, Valid Loss: 5.5288\n",
      "Epoch [16/50], Train Loss: 4.8094, Valid Loss: 5.5629\n",
      "Epoch [17/50], Train Loss: 4.6879, Valid Loss: 5.5532\n",
      "Epoch [18/50], Train Loss: 4.5649, Valid Loss: 5.3541\n",
      "Epoch [19/50], Train Loss: 4.4421, Valid Loss: 5.3329\n",
      "Epoch [20/50], Train Loss: 4.3080, Valid Loss: 5.3642\n",
      "Epoch [21/50], Train Loss: 4.1838, Valid Loss: 5.2110\n",
      "Epoch [22/50], Train Loss: 4.0423, Valid Loss: 5.1057\n",
      "Epoch [23/50], Train Loss: 3.8934, Valid Loss: 5.0372\n",
      "Epoch [24/50], Train Loss: 3.7800, Valid Loss: 4.9098\n",
      "Epoch [25/50], Train Loss: 3.6408, Valid Loss: 4.8598\n",
      "Epoch [26/50], Train Loss: 3.4739, Valid Loss: 4.7850\n",
      "Epoch [27/50], Train Loss: 3.3370, Valid Loss: 4.6009\n",
      "Epoch [28/50], Train Loss: 3.2041, Valid Loss: 4.6015\n",
      "Epoch [29/50], Train Loss: 3.0637, Valid Loss: 4.4971\n",
      "Epoch [30/50], Train Loss: 2.9482, Valid Loss: 4.4646\n",
      "Epoch [31/50], Train Loss: 2.8146, Valid Loss: 4.3783\n",
      "Epoch [32/50], Train Loss: 2.6868, Valid Loss: 4.2991\n",
      "Epoch [33/50], Train Loss: 2.5801, Valid Loss: 4.1353\n",
      "Epoch [34/50], Train Loss: 2.4721, Valid Loss: 4.1728\n",
      "Epoch [35/50], Train Loss: 2.3538, Valid Loss: 4.0616\n",
      "Epoch [36/50], Train Loss: 2.2757, Valid Loss: 3.9417\n",
      "Epoch [37/50], Train Loss: 2.1701, Valid Loss: 3.8825\n",
      "Epoch [38/50], Train Loss: 2.0601, Valid Loss: 3.8162\n",
      "Epoch [39/50], Train Loss: 2.0235, Valid Loss: 3.8955\n",
      "Epoch [40/50], Train Loss: 1.9201, Valid Loss: 3.8530\n",
      "Epoch [41/50], Train Loss: 1.8866, Valid Loss: 3.6913\n",
      "Epoch [42/50], Train Loss: 1.8295, Valid Loss: 3.6280\n",
      "Epoch [43/50], Train Loss: 1.7624, Valid Loss: 3.5682\n",
      "Epoch [44/50], Train Loss: 1.7066, Valid Loss: 3.7293\n",
      "Epoch [45/50], Train Loss: 1.7073, Valid Loss: 3.4949\n",
      "Epoch [46/50], Train Loss: 1.6036, Valid Loss: 3.4797\n",
      "Epoch [47/50], Train Loss: 1.5653, Valid Loss: 3.3458\n",
      "Epoch [48/50], Train Loss: 1.5383, Valid Loss: 3.4282\n",
      "Epoch [49/50], Train Loss: 1.5151, Valid Loss: 3.3162\n",
      "Epoch [50/50], Train Loss: 1.4343, Valid Loss: 3.4432\n",
      "Test Loss: 3.2746\n",
      "\n",
      "Testing Batch Size: 64\n",
      "Epoch [1/50], Train Loss: 6.4596, Valid Loss: 6.4374\n",
      "Epoch [2/50], Train Loss: 6.1344, Valid Loss: 6.1937\n",
      "Epoch [3/50], Train Loss: 6.0424, Valid Loss: 6.0937\n",
      "Epoch [4/50], Train Loss: 5.9226, Valid Loss: 6.2550\n",
      "Epoch [5/50], Train Loss: 5.8408, Valid Loss: 6.0213\n",
      "Epoch [6/50], Train Loss: 5.7681, Valid Loss: 6.1069\n",
      "Epoch [7/50], Train Loss: 5.7101, Valid Loss: 5.9991\n",
      "Epoch [8/50], Train Loss: 5.5933, Valid Loss: 6.0073\n",
      "Epoch [9/50], Train Loss: 5.4967, Valid Loss: 5.8492\n",
      "Epoch [10/50], Train Loss: 5.3755, Valid Loss: 5.7664\n",
      "Epoch [11/50], Train Loss: 5.2839, Valid Loss: 5.8374\n",
      "Epoch [12/50], Train Loss: 5.1711, Valid Loss: 5.6542\n",
      "Epoch [13/50], Train Loss: 5.0339, Valid Loss: 5.8044\n",
      "Epoch [14/50], Train Loss: 4.9168, Valid Loss: 5.7248\n",
      "Epoch [15/50], Train Loss: 4.7911, Valid Loss: 5.4947\n",
      "Epoch [16/50], Train Loss: 4.6925, Valid Loss: 5.4230\n",
      "Epoch [17/50], Train Loss: 4.5262, Valid Loss: 5.3171\n",
      "Epoch [18/50], Train Loss: 4.3867, Valid Loss: 5.2906\n",
      "Epoch [19/50], Train Loss: 4.2287, Valid Loss: 5.2737\n",
      "Epoch [20/50], Train Loss: 4.1012, Valid Loss: 5.0488\n",
      "Epoch [21/50], Train Loss: 3.9213, Valid Loss: 4.9415\n",
      "Epoch [22/50], Train Loss: 3.7193, Valid Loss: 4.8527\n",
      "Epoch [23/50], Train Loss: 3.5267, Valid Loss: 4.6893\n",
      "Epoch [24/50], Train Loss: 3.3620, Valid Loss: 4.6284\n",
      "Epoch [25/50], Train Loss: 3.1932, Valid Loss: 4.5132\n",
      "Epoch [26/50], Train Loss: 2.9907, Valid Loss: 4.3795\n",
      "Epoch [27/50], Train Loss: 2.7896, Valid Loss: 4.2933\n",
      "Epoch [28/50], Train Loss: 2.6200, Valid Loss: 4.2312\n",
      "Epoch [29/50], Train Loss: 2.4860, Valid Loss: 4.1257\n",
      "Epoch [30/50], Train Loss: 2.3207, Valid Loss: 3.9782\n",
      "Epoch [31/50], Train Loss: 2.1790, Valid Loss: 3.8725\n",
      "Epoch [32/50], Train Loss: 2.0497, Valid Loss: 3.7506\n",
      "Epoch [33/50], Train Loss: 1.9126, Valid Loss: 3.8094\n",
      "Epoch [34/50], Train Loss: 1.8361, Valid Loss: 3.6017\n",
      "Epoch [35/50], Train Loss: 1.7559, Valid Loss: 3.5804\n",
      "Epoch [36/50], Train Loss: 1.6528, Valid Loss: 3.6010\n",
      "Epoch [37/50], Train Loss: 1.6344, Valid Loss: 3.4786\n",
      "Epoch [38/50], Train Loss: 1.5224, Valid Loss: 3.4592\n",
      "Epoch [39/50], Train Loss: 1.5225, Valid Loss: 3.4303\n",
      "Epoch [40/50], Train Loss: 1.4418, Valid Loss: 3.4962\n",
      "Epoch [41/50], Train Loss: 1.4184, Valid Loss: 3.3364\n",
      "Epoch [42/50], Train Loss: 1.3408, Valid Loss: 3.3017\n",
      "Epoch [43/50], Train Loss: 1.3904, Valid Loss: 3.2735\n",
      "Epoch [44/50], Train Loss: 1.2508, Valid Loss: 3.2058\n",
      "Epoch [45/50], Train Loss: 1.2529, Valid Loss: 3.1922\n",
      "Epoch [46/50], Train Loss: 1.2558, Valid Loss: 3.1600\n",
      "Epoch [47/50], Train Loss: 1.2487, Valid Loss: 3.3530\n",
      "Epoch [48/50], Train Loss: 1.2079, Valid Loss: 3.1476\n",
      "Epoch [49/50], Train Loss: 1.1597, Valid Loss: 3.0357\n",
      "Epoch [50/50], Train Loss: 1.1070, Valid Loss: 3.0609\n",
      "Test Loss: 2.9797\n",
      "\n",
      "Testing Batch Size: 128\n",
      "Epoch [1/50], Train Loss: 6.4813, Valid Loss: 6.3543\n",
      "Epoch [2/50], Train Loss: 6.1015, Valid Loss: 6.5449\n",
      "Epoch [3/50], Train Loss: 6.0431, Valid Loss: 6.1644\n",
      "Epoch [4/50], Train Loss: 5.9096, Valid Loss: 6.1805\n",
      "Epoch [5/50], Train Loss: 5.8498, Valid Loss: 6.0359\n",
      "Epoch [6/50], Train Loss: 5.7944, Valid Loss: 5.9757\n",
      "Epoch [7/50], Train Loss: 5.6974, Valid Loss: 6.0420\n",
      "Epoch [8/50], Train Loss: 5.6336, Valid Loss: 5.8858\n",
      "Epoch [9/50], Train Loss: 5.5459, Valid Loss: 5.8528\n",
      "Epoch [10/50], Train Loss: 5.4546, Valid Loss: 5.8825\n",
      "Epoch [11/50], Train Loss: 5.3690, Valid Loss: 5.7699\n",
      "Epoch [12/50], Train Loss: 5.2228, Valid Loss: 5.7240\n",
      "Epoch [13/50], Train Loss: 5.1084, Valid Loss: 5.6259\n",
      "Epoch [14/50], Train Loss: 4.9933, Valid Loss: 5.5357\n",
      "Epoch [15/50], Train Loss: 4.8779, Valid Loss: 5.5148\n",
      "Epoch [16/50], Train Loss: 4.7376, Valid Loss: 5.3997\n",
      "Epoch [17/50], Train Loss: 4.5608, Valid Loss: 5.2963\n",
      "Epoch [18/50], Train Loss: 4.3906, Valid Loss: 5.3423\n",
      "Epoch [19/50], Train Loss: 4.2241, Valid Loss: 5.1246\n",
      "Epoch [20/50], Train Loss: 3.9864, Valid Loss: 4.9827\n",
      "Epoch [21/50], Train Loss: 3.7488, Valid Loss: 4.8618\n",
      "Epoch [22/50], Train Loss: 3.4957, Valid Loss: 4.6711\n",
      "Epoch [23/50], Train Loss: 3.2965, Valid Loss: 4.4634\n",
      "Epoch [24/50], Train Loss: 3.0044, Valid Loss: 4.3913\n",
      "Epoch [25/50], Train Loss: 2.7950, Valid Loss: 4.1514\n",
      "Epoch [26/50], Train Loss: 2.5493, Valid Loss: 4.0324\n",
      "Epoch [27/50], Train Loss: 2.3727, Valid Loss: 3.9376\n",
      "Epoch [28/50], Train Loss: 2.1921, Valid Loss: 3.7988\n",
      "Epoch [29/50], Train Loss: 2.0157, Valid Loss: 3.7089\n",
      "Epoch [30/50], Train Loss: 1.9184, Valid Loss: 3.6347\n",
      "Epoch [31/50], Train Loss: 1.7956, Valid Loss: 3.4790\n",
      "Epoch [32/50], Train Loss: 1.6684, Valid Loss: 3.3791\n",
      "Epoch [33/50], Train Loss: 1.5600, Valid Loss: 3.3236\n",
      "Epoch [34/50], Train Loss: 1.5118, Valid Loss: 3.3122\n",
      "Epoch [35/50], Train Loss: 1.4090, Valid Loss: 3.4079\n",
      "Epoch [36/50], Train Loss: 1.3962, Valid Loss: 3.2816\n",
      "Epoch [37/50], Train Loss: 1.3892, Valid Loss: 3.2832\n",
      "Epoch [38/50], Train Loss: 1.3632, Valid Loss: 3.2328\n",
      "Epoch [39/50], Train Loss: 1.2844, Valid Loss: 3.0994\n",
      "Epoch [40/50], Train Loss: 1.1613, Valid Loss: 2.9722\n",
      "Epoch [41/50], Train Loss: 1.1327, Valid Loss: 3.0766\n",
      "Epoch [42/50], Train Loss: 1.1803, Valid Loss: 3.1445\n",
      "Epoch [43/50], Train Loss: 1.1396, Valid Loss: 2.9937\n",
      "Epoch [44/50], Train Loss: 1.0925, Valid Loss: 3.0691\n",
      "Epoch [45/50], Train Loss: 1.1314, Valid Loss: 2.9384\n",
      "Epoch [46/50], Train Loss: 1.0610, Valid Loss: 2.9029\n",
      "Epoch [47/50], Train Loss: 1.0140, Valid Loss: 3.0016\n",
      "Epoch [48/50], Train Loss: 1.0682, Valid Loss: 2.8326\n",
      "Epoch [49/50], Train Loss: 0.9426, Valid Loss: 2.8662\n",
      "Epoch [50/50], Train Loss: 1.0159, Valid Loss: 2.9113\n",
      "Test Loss: 2.8506\n",
      "\n",
      "Testing Batch Size: 256\n",
      "Epoch [1/50], Train Loss: 6.5889, Valid Loss: 6.2734\n",
      "Epoch [2/50], Train Loss: 6.0798, Valid Loss: 6.1103\n",
      "Epoch [3/50], Train Loss: 5.9996, Valid Loss: 6.1278\n",
      "Epoch [4/50], Train Loss: 5.9172, Valid Loss: 6.0771\n",
      "Epoch [5/50], Train Loss: 5.8717, Valid Loss: 6.0715\n",
      "Epoch [6/50], Train Loss: 5.7977, Valid Loss: 6.0116\n",
      "Epoch [7/50], Train Loss: 5.7250, Valid Loss: 5.9078\n",
      "Epoch [8/50], Train Loss: 5.6325, Valid Loss: 5.8690\n",
      "Epoch [9/50], Train Loss: 5.5833, Valid Loss: 5.8214\n",
      "Epoch [10/50], Train Loss: 5.4728, Valid Loss: 5.8157\n",
      "Epoch [11/50], Train Loss: 5.4046, Valid Loss: 5.7457\n",
      "Epoch [12/50], Train Loss: 5.3133, Valid Loss: 5.7531\n",
      "Epoch [13/50], Train Loss: 5.2390, Valid Loss: 5.7156\n",
      "Epoch [14/50], Train Loss: 5.1319, Valid Loss: 5.6057\n",
      "Epoch [15/50], Train Loss: 4.9964, Valid Loss: 5.5607\n",
      "Epoch [16/50], Train Loss: 4.8153, Valid Loss: 5.4352\n",
      "Epoch [17/50], Train Loss: 4.7302, Valid Loss: 5.4054\n",
      "Epoch [18/50], Train Loss: 4.5557, Valid Loss: 5.1835\n",
      "Epoch [19/50], Train Loss: 4.3513, Valid Loss: 5.2197\n",
      "Epoch [20/50], Train Loss: 4.1468, Valid Loss: 5.1349\n",
      "Epoch [21/50], Train Loss: 3.9203, Valid Loss: 4.9356\n",
      "Epoch [22/50], Train Loss: 3.7446, Valid Loss: 4.8019\n",
      "Epoch [23/50], Train Loss: 3.4196, Valid Loss: 4.6366\n",
      "Epoch [24/50], Train Loss: 3.2019, Valid Loss: 4.4403\n",
      "Epoch [25/50], Train Loss: 2.9070, Valid Loss: 4.2429\n",
      "Epoch [26/50], Train Loss: 2.6603, Valid Loss: 4.0546\n",
      "Epoch [27/50], Train Loss: 2.4510, Valid Loss: 4.0040\n",
      "Epoch [28/50], Train Loss: 2.2397, Valid Loss: 3.8020\n",
      "Epoch [29/50], Train Loss: 2.0817, Valid Loss: 3.6345\n",
      "Epoch [30/50], Train Loss: 1.8907, Valid Loss: 3.4820\n",
      "Epoch [31/50], Train Loss: 1.6934, Valid Loss: 3.4245\n",
      "Epoch [32/50], Train Loss: 1.5747, Valid Loss: 3.3810\n",
      "Epoch [33/50], Train Loss: 1.5168, Valid Loss: 3.4169\n",
      "Epoch [34/50], Train Loss: 1.4364, Valid Loss: 3.2778\n",
      "Epoch [35/50], Train Loss: 1.3636, Valid Loss: 3.1784\n",
      "Epoch [36/50], Train Loss: 1.3075, Valid Loss: 3.2513\n",
      "Epoch [37/50], Train Loss: 1.2531, Valid Loss: 3.0686\n",
      "Epoch [38/50], Train Loss: 1.1863, Valid Loss: 3.0053\n",
      "Epoch [39/50], Train Loss: 1.1672, Valid Loss: 3.0037\n",
      "Epoch [40/50], Train Loss: 1.1459, Valid Loss: 3.0469\n",
      "Epoch [41/50], Train Loss: 1.0650, Valid Loss: 2.9477\n",
      "Epoch [42/50], Train Loss: 1.0919, Valid Loss: 2.8489\n",
      "Epoch [43/50], Train Loss: 0.9979, Valid Loss: 2.9092\n",
      "Epoch [44/50], Train Loss: 1.0474, Valid Loss: 2.9853\n",
      "Epoch [45/50], Train Loss: 0.9668, Valid Loss: 2.7567\n",
      "Epoch [46/50], Train Loss: 0.8729, Valid Loss: 2.7784\n",
      "Epoch [47/50], Train Loss: 0.8827, Valid Loss: 2.8051\n",
      "Epoch [48/50], Train Loss: 0.9324, Valid Loss: 2.9104\n",
      "Epoch [49/50], Train Loss: 1.0154, Valid Loss: 2.9828\n",
      "Epoch [50/50], Train Loss: 1.1095, Valid Loss: 3.0428\n",
      "Test Loss: 2.9443\n",
      "\n",
      "Testing Batch Size: 512\n",
      "Epoch [1/50], Train Loss: 7.0993, Valid Loss: 7.5677\n",
      "Epoch [2/50], Train Loss: 6.1467, Valid Loss: 6.2683\n",
      "Epoch [3/50], Train Loss: 6.0022, Valid Loss: 6.1688\n",
      "Epoch [4/50], Train Loss: 5.9520, Valid Loss: 6.1352\n",
      "Epoch [5/50], Train Loss: 5.8816, Valid Loss: 6.1319\n",
      "Epoch [6/50], Train Loss: 5.8424, Valid Loss: 6.0784\n",
      "Epoch [7/50], Train Loss: 5.7489, Valid Loss: 6.0377\n",
      "Epoch [8/50], Train Loss: 5.7355, Valid Loss: 6.0000\n",
      "Epoch [9/50], Train Loss: 5.7194, Valid Loss: 6.1585\n",
      "Epoch [10/50], Train Loss: 5.6144, Valid Loss: 5.9923\n",
      "Epoch [11/50], Train Loss: 5.5394, Valid Loss: 5.7975\n",
      "Epoch [12/50], Train Loss: 5.4719, Valid Loss: 5.8216\n",
      "Epoch [13/50], Train Loss: 5.3574, Valid Loss: 5.8732\n",
      "Epoch [14/50], Train Loss: 5.3250, Valid Loss: 5.7980\n",
      "Epoch [15/50], Train Loss: 5.1898, Valid Loss: 5.8001\n",
      "Epoch [16/50], Train Loss: 5.1480, Valid Loss: 5.6645\n",
      "Epoch [17/50], Train Loss: 5.0199, Valid Loss: 5.6084\n",
      "Epoch [18/50], Train Loss: 4.8666, Valid Loss: 5.6002\n",
      "Epoch [19/50], Train Loss: 4.7428, Valid Loss: 5.5978\n",
      "Epoch [20/50], Train Loss: 4.5901, Valid Loss: 5.4111\n",
      "Epoch [21/50], Train Loss: 4.3915, Valid Loss: 5.4206\n",
      "Epoch [22/50], Train Loss: 4.2569, Valid Loss: 5.2871\n",
      "Epoch [23/50], Train Loss: 4.0636, Valid Loss: 5.1306\n",
      "Epoch [24/50], Train Loss: 3.8702, Valid Loss: 5.0012\n",
      "Epoch [25/50], Train Loss: 3.6330, Valid Loss: 4.8820\n",
      "Epoch [26/50], Train Loss: 3.3611, Valid Loss: 4.8101\n",
      "Epoch [27/50], Train Loss: 3.1932, Valid Loss: 4.5767\n",
      "Epoch [28/50], Train Loss: 2.8525, Valid Loss: 4.4072\n",
      "Epoch [29/50], Train Loss: 2.6560, Valid Loss: 4.2975\n",
      "Epoch [30/50], Train Loss: 2.4011, Valid Loss: 4.0231\n",
      "Epoch [31/50], Train Loss: 2.1630, Valid Loss: 3.8085\n",
      "Epoch [32/50], Train Loss: 1.9871, Valid Loss: 3.7496\n",
      "Epoch [33/50], Train Loss: 1.8135, Valid Loss: 3.6197\n",
      "Epoch [34/50], Train Loss: 1.6695, Valid Loss: 3.5289\n",
      "Epoch [35/50], Train Loss: 1.5932, Valid Loss: 3.4389\n",
      "Epoch [36/50], Train Loss: 1.4831, Valid Loss: 3.3529\n",
      "Epoch [37/50], Train Loss: 1.3228, Valid Loss: 3.3823\n",
      "Epoch [38/50], Train Loss: 1.2403, Valid Loss: 3.2726\n",
      "Epoch [39/50], Train Loss: 1.1890, Valid Loss: 3.0848\n",
      "Epoch [40/50], Train Loss: 1.1257, Valid Loss: 3.0138\n",
      "Epoch [41/50], Train Loss: 1.0241, Valid Loss: 3.1224\n",
      "Epoch [42/50], Train Loss: 0.9863, Valid Loss: 2.8845\n",
      "Epoch [43/50], Train Loss: 0.9771, Valid Loss: 2.9723\n",
      "Epoch [44/50], Train Loss: 0.9116, Valid Loss: 2.9364\n",
      "Epoch [45/50], Train Loss: 0.8932, Valid Loss: 2.8822\n",
      "Epoch [46/50], Train Loss: 0.8662, Valid Loss: 2.7442\n",
      "Epoch [47/50], Train Loss: 0.8168, Valid Loss: 2.8656\n",
      "Epoch [48/50], Train Loss: 0.7891, Valid Loss: 2.8346\n",
      "Epoch [49/50], Train Loss: 0.7508, Valid Loss: 2.7952\n",
      "Epoch [50/50], Train Loss: 0.7615, Valid Loss: 2.8389\n",
      "Test Loss: 2.7572\n",
      "Best Batch Size: 512 with Validation Loss: 2.8389\n"
     ]
    }
   ],
   "source": [
    "# Normalized, try different batch size\n",
    "class MultiModalDeepChessDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.df.iloc[idx]['king_safety_tensor']\n",
    "        x2 = self.df.iloc[idx]['material_tensor']\n",
    "        x3 = self.df.iloc[idx]['space_tensor']\n",
    "        x4 = self.df.iloc[idx]['pawn_structure_tensor']\n",
    "        x5 = self.df.iloc[idx]['development_tensor']\n",
    "        label = self.df.iloc[idx]['evaluation']\n",
    "\n",
    "        return x1, x2, x3, x4, x5, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "train_dataset = MultiModalDeepChessDataset(train_df)\n",
    "valid_dataset = MultiModalDeepChessDataset(valid_df)\n",
    "test_dataset = MultiModalDeepChessDataset(test_df)\n",
    "\n",
    "def create_data_loaders(batch_size):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "class MMDBN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MMDBN, self).__init__()\n",
    "        self.mmdbn = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mmdbn(x)\n",
    "    \n",
    "class MultiModalDeepChess(nn.Module):\n",
    "    def __init__(self, input_size_1, input_size_2, input_size_3, input_size_4, input_size_5):\n",
    "        super(MultiModalDeepChess, self).__init__()\n",
    "        self.branch_1 = MMDBN(input_size_1)\n",
    "        self.branch_2 = MMDBN(input_size_2)\n",
    "        self.branch_3 = MMDBN(input_size_3)\n",
    "        self.branch_4 = MMDBN(input_size_4)\n",
    "        self.branch_5 = MMDBN(input_size_5)\n",
    "\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Linear(1000, 800),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(800, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4, x5):\n",
    "        x1 = self.branch_1(x1)\n",
    "        x2 = self.branch_2(x2)\n",
    "        x3 = self.branch_3(x3)\n",
    "        x4 = self.branch_4(x4)\n",
    "        x5 = self.branch_5(x5)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "\n",
    "        return self.merge_layers(x)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_best_loss(file_path=\"../output/best_loss.txt\"):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read().strip()\n",
    "            if content == \"\":\n",
    "                return float(\"inf\")\n",
    "            return float(content)\n",
    "        \n",
    "    return float(\"inf\")\n",
    "\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, epochs):\n",
    "    best_valid_loss = load_best_loss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for x1, x2, x3, x4, x5, labels in train_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        valid_loss = validate(model, valid_loader, criterion)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Valid Loss: {valid_loss:.4f}')\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"../output/model.pt\")\n",
    "\n",
    "            with open(\"../output/best_loss.txt\", \"w\") as f:\n",
    "                f.write(str(best_valid_loss))\n",
    "\n",
    "            print(f'Model saved with Validation Loss: {valid_loss:.4f}')\n",
    "    \n",
    "    return train_loss, valid_loss\n",
    "\n",
    "def validate(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, x5, labels in valid_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    return valid_loss / len(valid_loader)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, x5, labels in test_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}')\n",
    "\n",
    "input_size_1 = len(df['king_safety_tensor'][0])\n",
    "input_size_2 = len(df['material_tensor'][0])\n",
    "input_size_3 = len(df['space_tensor'][0])\n",
    "input_size_4 = len(df['pawn_structure_tensor'][0])\n",
    "input_size_5 = len(df['development_tensor'][0])\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "batch_sizes = [32, 64, 128, 256, 512]\n",
    "batch_performance = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nTesting Batch Size: {batch_size}\")\n",
    "    \n",
    "    train_loader, valid_loader, test_loader = create_data_loaders(batch_size)\n",
    "    model = MultiModalDeepChess(input_size_1, input_size_2, input_size_3, input_size_4, input_size_5).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss, valid_loss = train(model, train_loader, valid_loader, criterion, optimizer, num_epochs)\n",
    "    test(model, test_loader, criterion)\n",
    "\n",
    "    batch_performance[batch_size] = valid_loss\n",
    "\n",
    "best_batch_size = min(batch_performance, key=batch_performance.get)\n",
    "print(f\"Best Batch Size: {best_batch_size} with Validation Loss: {batch_performance[best_batch_size]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Optimizer: Adam\n",
      "Epoch [1/50], Train Loss: 6.8157, Valid Loss: 6.2757\n",
      "Epoch [2/50], Train Loss: 6.0861, Valid Loss: 6.1105\n",
      "Epoch [3/50], Train Loss: 5.9739, Valid Loss: 6.1915\n",
      "Epoch [4/50], Train Loss: 5.9627, Valid Loss: 6.5260\n",
      "Epoch [5/50], Train Loss: 5.9136, Valid Loss: 6.1383\n",
      "Epoch [6/50], Train Loss: 5.8459, Valid Loss: 6.3884\n",
      "Epoch [7/50], Train Loss: 5.7877, Valid Loss: 5.9343\n",
      "Epoch [8/50], Train Loss: 5.6864, Valid Loss: 5.9553\n",
      "Epoch [9/50], Train Loss: 5.6504, Valid Loss: 5.8898\n",
      "Epoch [10/50], Train Loss: 5.5491, Valid Loss: 5.8970\n",
      "Epoch [11/50], Train Loss: 5.4958, Valid Loss: 5.8794\n",
      "Epoch [12/50], Train Loss: 5.4177, Valid Loss: 5.7758\n",
      "Epoch [13/50], Train Loss: 5.2721, Valid Loss: 6.3478\n",
      "Epoch [14/50], Train Loss: 5.2572, Valid Loss: 5.7784\n",
      "Epoch [15/50], Train Loss: 5.1216, Valid Loss: 5.7644\n",
      "Epoch [16/50], Train Loss: 4.9564, Valid Loss: 5.6588\n",
      "Epoch [17/50], Train Loss: 4.8749, Valid Loss: 5.5415\n",
      "Epoch [18/50], Train Loss: 4.7473, Valid Loss: 5.4829\n",
      "Epoch [19/50], Train Loss: 4.6656, Valid Loss: 5.4378\n",
      "Epoch [20/50], Train Loss: 4.4728, Valid Loss: 5.3574\n",
      "Epoch [21/50], Train Loss: 4.3195, Valid Loss: 5.2111\n",
      "Epoch [22/50], Train Loss: 4.1587, Valid Loss: 5.1401\n",
      "Epoch [23/50], Train Loss: 3.9556, Valid Loss: 5.0937\n",
      "Epoch [24/50], Train Loss: 3.6862, Valid Loss: 5.0358\n",
      "Epoch [25/50], Train Loss: 3.4845, Valid Loss: 4.8412\n",
      "Epoch [26/50], Train Loss: 3.1968, Valid Loss: 4.7662\n",
      "Epoch [27/50], Train Loss: 2.9834, Valid Loss: 4.4248\n",
      "Epoch [28/50], Train Loss: 2.7396, Valid Loss: 4.2494\n",
      "Epoch [29/50], Train Loss: 2.5098, Valid Loss: 4.0823\n",
      "Epoch [30/50], Train Loss: 2.3174, Valid Loss: 3.9448\n",
      "Epoch [31/50], Train Loss: 2.0990, Valid Loss: 3.8659\n",
      "Epoch [32/50], Train Loss: 1.9240, Valid Loss: 3.6291\n",
      "Epoch [33/50], Train Loss: 1.8246, Valid Loss: 3.5242\n",
      "Epoch [34/50], Train Loss: 1.6519, Valid Loss: 3.4852\n",
      "Epoch [35/50], Train Loss: 1.5163, Valid Loss: 3.4053\n",
      "Epoch [36/50], Train Loss: 1.3644, Valid Loss: 3.2090\n",
      "Epoch [37/50], Train Loss: 1.2837, Valid Loss: 3.2389\n",
      "Epoch [38/50], Train Loss: 1.1998, Valid Loss: 3.1419\n",
      "Epoch [39/50], Train Loss: 1.1777, Valid Loss: 3.0670\n",
      "Epoch [40/50], Train Loss: 1.1404, Valid Loss: 3.0840\n",
      "Epoch [41/50], Train Loss: 1.0523, Valid Loss: 3.0492\n",
      "Epoch [42/50], Train Loss: 1.0639, Valid Loss: 2.9447\n",
      "Epoch [43/50], Train Loss: 0.9907, Valid Loss: 2.9573\n",
      "Epoch [44/50], Train Loss: 0.9408, Valid Loss: 2.8570\n",
      "Epoch [45/50], Train Loss: 0.8962, Valid Loss: 2.8410\n",
      "Epoch [46/50], Train Loss: 0.8381, Valid Loss: 2.8165\n",
      "Epoch [47/50], Train Loss: 0.8350, Valid Loss: 2.8267\n",
      "Epoch [48/50], Train Loss: 0.7479, Valid Loss: 2.7377\n",
      "Epoch [49/50], Train Loss: 0.7469, Valid Loss: 2.7141\n",
      "Epoch [50/50], Train Loss: 0.7875, Valid Loss: 2.7095\n",
      "Test Loss: 2.5965\n",
      "\n",
      "Testing Optimizer: SGD\n",
      "Epoch [1/50], Train Loss: 10.4735, Valid Loss: 10.7202\n",
      "Epoch [2/50], Train Loss: 10.4186, Valid Loss: 10.7127\n",
      "Epoch [3/50], Train Loss: 10.4239, Valid Loss: 10.6793\n",
      "Epoch [4/50], Train Loss: 10.2481, Valid Loss: 10.0181\n",
      "Epoch [5/50], Train Loss: 8.0706, Valid Loss: 7.5288\n",
      "Epoch [6/50], Train Loss: 7.2211, Valid Loss: 7.1634\n",
      "Epoch [7/50], Train Loss: 6.8838, Valid Loss: 6.9245\n",
      "Epoch [8/50], Train Loss: 6.6751, Valid Loss: 6.5998\n",
      "Epoch [9/50], Train Loss: 6.3818, Valid Loss: 6.3925\n",
      "Epoch [10/50], Train Loss: 6.2078, Valid Loss: 6.2937\n",
      "Epoch [11/50], Train Loss: 6.1788, Valid Loss: 6.2636\n",
      "Epoch [12/50], Train Loss: 6.1269, Valid Loss: 6.3323\n",
      "Epoch [13/50], Train Loss: 6.1205, Valid Loss: 6.2088\n",
      "Epoch [14/50], Train Loss: 6.0567, Valid Loss: 6.2142\n",
      "Epoch [15/50], Train Loss: 6.0084, Valid Loss: 6.1740\n",
      "Epoch [16/50], Train Loss: 5.9938, Valid Loss: 6.1412\n",
      "Epoch [17/50], Train Loss: 5.9771, Valid Loss: 6.1411\n",
      "Epoch [18/50], Train Loss: 5.9290, Valid Loss: 6.1400\n",
      "Epoch [19/50], Train Loss: 5.9130, Valid Loss: 6.0886\n",
      "Epoch [20/50], Train Loss: 5.9184, Valid Loss: 6.0625\n",
      "Epoch [21/50], Train Loss: 5.8759, Valid Loss: 6.0917\n",
      "Epoch [22/50], Train Loss: 5.8717, Valid Loss: 6.1124\n",
      "Epoch [23/50], Train Loss: 5.9027, Valid Loss: 6.0505\n",
      "Epoch [24/50], Train Loss: 5.8254, Valid Loss: 6.0288\n",
      "Epoch [25/50], Train Loss: 5.8633, Valid Loss: 6.0188\n",
      "Epoch [26/50], Train Loss: 5.7939, Valid Loss: 6.0109\n",
      "Epoch [27/50], Train Loss: 5.7891, Valid Loss: 6.1433\n",
      "Epoch [28/50], Train Loss: 5.7753, Valid Loss: 6.1002\n",
      "Epoch [29/50], Train Loss: 5.7892, Valid Loss: 6.0266\n",
      "Epoch [30/50], Train Loss: 5.7413, Valid Loss: 6.0725\n",
      "Epoch [31/50], Train Loss: 5.7809, Valid Loss: 5.9940\n",
      "Epoch [32/50], Train Loss: 5.7468, Valid Loss: 6.0181\n",
      "Epoch [33/50], Train Loss: 5.7018, Valid Loss: 6.0503\n",
      "Epoch [34/50], Train Loss: 5.6972, Valid Loss: 5.9294\n",
      "Epoch [35/50], Train Loss: 5.6482, Valid Loss: 5.9590\n",
      "Epoch [36/50], Train Loss: 5.6375, Valid Loss: 5.9454\n",
      "Epoch [37/50], Train Loss: 5.6163, Valid Loss: 6.5326\n",
      "Epoch [38/50], Train Loss: 5.6376, Valid Loss: 5.9363\n",
      "Epoch [39/50], Train Loss: 5.5961, Valid Loss: 6.0984\n",
      "Epoch [40/50], Train Loss: 5.5314, Valid Loss: 6.3953\n",
      "Epoch [41/50], Train Loss: 5.5430, Valid Loss: 5.8962\n",
      "Epoch [42/50], Train Loss: 5.5267, Valid Loss: 5.9994\n",
      "Epoch [43/50], Train Loss: 5.4709, Valid Loss: 5.8353\n",
      "Epoch [44/50], Train Loss: 5.4203, Valid Loss: 5.9460\n",
      "Epoch [45/50], Train Loss: 5.3724, Valid Loss: 5.7601\n",
      "Epoch [46/50], Train Loss: 5.4035, Valid Loss: 5.8086\n",
      "Epoch [47/50], Train Loss: 5.3271, Valid Loss: 6.0658\n",
      "Epoch [48/50], Train Loss: 5.3308, Valid Loss: 5.7751\n",
      "Epoch [49/50], Train Loss: 5.2823, Valid Loss: 5.7651\n",
      "Epoch [50/50], Train Loss: 5.2208, Valid Loss: 5.8034\n",
      "Test Loss: 5.6289\n",
      "\n",
      "Testing Optimizer: AdamW\n",
      "Epoch [1/50], Train Loss: 6.9262, Valid Loss: 6.2862\n",
      "Epoch [2/50], Train Loss: 6.1113, Valid Loss: 6.2045\n",
      "Epoch [3/50], Train Loss: 6.0050, Valid Loss: 6.1317\n",
      "Epoch [4/50], Train Loss: 5.9044, Valid Loss: 6.0993\n",
      "Epoch [5/50], Train Loss: 5.8691, Valid Loss: 6.1277\n",
      "Epoch [6/50], Train Loss: 5.8272, Valid Loss: 6.1686\n",
      "Epoch [7/50], Train Loss: 5.7793, Valid Loss: 6.1927\n",
      "Epoch [8/50], Train Loss: 5.7025, Valid Loss: 6.0216\n",
      "Epoch [9/50], Train Loss: 5.7117, Valid Loss: 5.9261\n",
      "Epoch [10/50], Train Loss: 5.6091, Valid Loss: 5.8986\n",
      "Epoch [11/50], Train Loss: 5.5410, Valid Loss: 5.9329\n",
      "Epoch [12/50], Train Loss: 5.4315, Valid Loss: 5.8621\n",
      "Epoch [13/50], Train Loss: 5.3872, Valid Loss: 5.9536\n",
      "Epoch [14/50], Train Loss: 5.3045, Valid Loss: 5.8402\n",
      "Epoch [15/50], Train Loss: 5.2359, Valid Loss: 5.9386\n",
      "Epoch [16/50], Train Loss: 5.1104, Valid Loss: 5.7635\n",
      "Epoch [17/50], Train Loss: 5.0442, Valid Loss: 5.6106\n",
      "Epoch [18/50], Train Loss: 4.9145, Valid Loss: 5.5740\n",
      "Epoch [19/50], Train Loss: 4.8043, Valid Loss: 5.5264\n",
      "Epoch [20/50], Train Loss: 4.6622, Valid Loss: 5.4610\n",
      "Epoch [21/50], Train Loss: 4.5486, Valid Loss: 5.3116\n",
      "Epoch [22/50], Train Loss: 4.3830, Valid Loss: 5.2221\n",
      "Epoch [23/50], Train Loss: 4.1617, Valid Loss: 5.1363\n",
      "Epoch [24/50], Train Loss: 3.9606, Valid Loss: 5.4134\n",
      "Epoch [25/50], Train Loss: 3.7914, Valid Loss: 4.8827\n",
      "Epoch [26/50], Train Loss: 3.5610, Valid Loss: 4.8703\n",
      "Epoch [27/50], Train Loss: 3.3350, Valid Loss: 4.5773\n",
      "Epoch [28/50], Train Loss: 3.0856, Valid Loss: 4.3972\n",
      "Epoch [29/50], Train Loss: 2.7956, Valid Loss: 4.3219\n",
      "Epoch [30/50], Train Loss: 2.5793, Valid Loss: 4.1127\n",
      "Epoch [31/50], Train Loss: 2.3740, Valid Loss: 3.9719\n",
      "Epoch [32/50], Train Loss: 2.1447, Valid Loss: 3.9705\n",
      "Epoch [33/50], Train Loss: 2.0580, Valid Loss: 3.7995\n",
      "Epoch [34/50], Train Loss: 1.8740, Valid Loss: 3.5708\n",
      "Epoch [35/50], Train Loss: 1.6956, Valid Loss: 3.4905\n",
      "Epoch [36/50], Train Loss: 1.5674, Valid Loss: 3.3202\n",
      "Epoch [37/50], Train Loss: 1.4594, Valid Loss: 3.2891\n",
      "Epoch [38/50], Train Loss: 1.3782, Valid Loss: 3.3256\n",
      "Epoch [39/50], Train Loss: 1.2970, Valid Loss: 3.1599\n",
      "Epoch [40/50], Train Loss: 1.2217, Valid Loss: 3.1675\n",
      "Epoch [41/50], Train Loss: 1.1996, Valid Loss: 3.1133\n",
      "Epoch [42/50], Train Loss: 1.1315, Valid Loss: 3.0392\n",
      "Epoch [43/50], Train Loss: 1.0269, Valid Loss: 3.0416\n",
      "Epoch [44/50], Train Loss: 0.9798, Valid Loss: 3.0333\n",
      "Epoch [45/50], Train Loss: 0.9850, Valid Loss: 2.8975\n",
      "Epoch [46/50], Train Loss: 0.9331, Valid Loss: 2.8967\n",
      "Epoch [47/50], Train Loss: 0.9388, Valid Loss: 2.8887\n",
      "Epoch [48/50], Train Loss: 0.8809, Valid Loss: 2.9139\n",
      "Epoch [49/50], Train Loss: 0.8796, Valid Loss: 2.9002\n",
      "Epoch [50/50], Train Loss: 0.8933, Valid Loss: 2.7682\n",
      "Test Loss: 2.6619\n",
      "\n",
      "Testing Optimizer: RMSprop\n",
      "Epoch [1/50], Train Loss: 8.5424, Valid Loss: 8.3948\n",
      "Epoch [2/50], Train Loss: 6.4271, Valid Loss: 6.6985\n",
      "Epoch [3/50], Train Loss: 6.2339, Valid Loss: 6.5057\n",
      "Epoch [4/50], Train Loss: 6.1474, Valid Loss: 11.0434\n",
      "Epoch [5/50], Train Loss: 6.2644, Valid Loss: 6.1896\n",
      "Epoch [6/50], Train Loss: 5.9599, Valid Loss: 7.2036\n",
      "Epoch [7/50], Train Loss: 5.9873, Valid Loss: 6.4760\n",
      "Epoch [8/50], Train Loss: 5.8907, Valid Loss: 6.1249\n",
      "Epoch [9/50], Train Loss: 5.8088, Valid Loss: 6.1530\n",
      "Epoch [10/50], Train Loss: 5.7069, Valid Loss: 6.5556\n",
      "Epoch [11/50], Train Loss: 5.6587, Valid Loss: 5.9399\n",
      "Epoch [12/50], Train Loss: 5.5515, Valid Loss: 6.3090\n",
      "Epoch [13/50], Train Loss: 5.4351, Valid Loss: 6.8670\n",
      "Epoch [14/50], Train Loss: 5.4184, Valid Loss: 6.3503\n",
      "Epoch [15/50], Train Loss: 5.2557, Valid Loss: 6.2676\n",
      "Epoch [16/50], Train Loss: 5.1236, Valid Loss: 6.0407\n",
      "Epoch [17/50], Train Loss: 4.9987, Valid Loss: 5.9073\n",
      "Epoch [18/50], Train Loss: 4.8519, Valid Loss: 6.2899\n",
      "Epoch [19/50], Train Loss: 4.6765, Valid Loss: 5.6525\n",
      "Epoch [20/50], Train Loss: 4.5494, Valid Loss: 5.8134\n",
      "Epoch [21/50], Train Loss: 4.3021, Valid Loss: 6.1585\n",
      "Epoch [22/50], Train Loss: 4.0439, Valid Loss: 6.0172\n",
      "Epoch [23/50], Train Loss: 3.8268, Valid Loss: 5.3029\n",
      "Epoch [24/50], Train Loss: 3.5527, Valid Loss: 5.6048\n",
      "Epoch [25/50], Train Loss: 3.3046, Valid Loss: 6.0330\n",
      "Epoch [26/50], Train Loss: 3.0498, Valid Loss: 5.3645\n",
      "Epoch [27/50], Train Loss: 2.7959, Valid Loss: 4.5012\n",
      "Epoch [28/50], Train Loss: 2.5428, Valid Loss: 4.6137\n",
      "Epoch [29/50], Train Loss: 2.3090, Valid Loss: 4.9878\n",
      "Epoch [30/50], Train Loss: 2.1681, Valid Loss: 4.6833\n",
      "Epoch [31/50], Train Loss: 2.0044, Valid Loss: 4.2922\n",
      "Epoch [32/50], Train Loss: 1.7873, Valid Loss: 3.6907\n",
      "Epoch [33/50], Train Loss: 1.7481, Valid Loss: 3.9738\n",
      "Epoch [34/50], Train Loss: 1.5777, Valid Loss: 4.0313\n",
      "Epoch [35/50], Train Loss: 1.4851, Valid Loss: 3.5066\n",
      "Epoch [36/50], Train Loss: 1.3842, Valid Loss: 4.1989\n",
      "Epoch [37/50], Train Loss: 1.3662, Valid Loss: 3.5935\n",
      "Epoch [38/50], Train Loss: 1.2622, Valid Loss: 4.0213\n",
      "Epoch [39/50], Train Loss: 1.2577, Valid Loss: 4.0519\n",
      "Epoch [40/50], Train Loss: 1.1324, Valid Loss: 3.4639\n",
      "Epoch [41/50], Train Loss: 1.1886, Valid Loss: 3.2625\n",
      "Epoch [42/50], Train Loss: 1.0904, Valid Loss: 3.8240\n",
      "Epoch [43/50], Train Loss: 1.0578, Valid Loss: 4.0074\n",
      "Epoch [44/50], Train Loss: 1.0541, Valid Loss: 3.5319\n",
      "Epoch [45/50], Train Loss: 0.9861, Valid Loss: 2.9561\n",
      "Epoch [46/50], Train Loss: 0.9636, Valid Loss: 3.1429\n",
      "Epoch [47/50], Train Loss: 0.9477, Valid Loss: 3.0044\n",
      "Epoch [48/50], Train Loss: 0.9745, Valid Loss: 3.0452\n",
      "Epoch [49/50], Train Loss: 0.8793, Valid Loss: 2.9678\n",
      "Epoch [50/50], Train Loss: 0.9096, Valid Loss: 2.8742\n",
      "Test Loss: 2.7960\n",
      "Best Batch Size: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ") with Validation Loss: 2.7095\n"
     ]
    }
   ],
   "source": [
    "# Normalized, try different optimizer\n",
    "class MultiModalDeepChessDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.df.iloc[idx]['king_safety_tensor']\n",
    "        x2 = self.df.iloc[idx]['material_tensor']\n",
    "        x3 = self.df.iloc[idx]['space_tensor']\n",
    "        x4 = self.df.iloc[idx]['pawn_structure_tensor']\n",
    "        x5 = self.df.iloc[idx]['development_tensor']\n",
    "        label = self.df.iloc[idx]['evaluation']\n",
    "\n",
    "        return x1, x2, x3, x4, x5, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "train_dataset = MultiModalDeepChessDataset(train_df)\n",
    "valid_dataset = MultiModalDeepChessDataset(valid_df)\n",
    "test_dataset = MultiModalDeepChessDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=512, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "\n",
    "class MMDBN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MMDBN, self).__init__()\n",
    "        self.mmdbn = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mmdbn(x)\n",
    "    \n",
    "class MultiModalDeepChess(nn.Module):\n",
    "    def __init__(self, input_size_1, input_size_2, input_size_3, input_size_4, input_size_5):\n",
    "        super(MultiModalDeepChess, self).__init__()\n",
    "        self.branch_1 = MMDBN(input_size_1)\n",
    "        self.branch_2 = MMDBN(input_size_2)\n",
    "        self.branch_3 = MMDBN(input_size_3)\n",
    "        self.branch_4 = MMDBN(input_size_4)\n",
    "        self.branch_5 = MMDBN(input_size_5)\n",
    "\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Linear(1000, 800),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(800, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4, x5):\n",
    "        x1 = self.branch_1(x1)\n",
    "        x2 = self.branch_2(x2)\n",
    "        x3 = self.branch_3(x3)\n",
    "        x4 = self.branch_4(x4)\n",
    "        x5 = self.branch_5(x5)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "\n",
    "        return self.merge_layers(x)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_best_loss(file_path=\"../output/best_loss.txt\"):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read().strip()\n",
    "            if content == \"\":\n",
    "                return float(\"inf\")\n",
    "            return float(content)\n",
    "        \n",
    "    return float(\"inf\")\n",
    "\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, epochs):\n",
    "    best_valid_loss = load_best_loss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for x1, x2, x3, x4, x5, labels in train_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        valid_loss = validate(model, valid_loader, criterion)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Valid Loss: {valid_loss:.4f}')\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"../output/model.pt\")\n",
    "\n",
    "            with open(\"../output/best_loss.txt\", \"w\") as f:\n",
    "                f.write(str(best_valid_loss))\n",
    "\n",
    "            print(f'Model saved with Validation Loss: {valid_loss:.4f}')\n",
    "    \n",
    "    return train_loss, valid_loss\n",
    "\n",
    "def validate(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, x5, labels in valid_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    return valid_loss / len(valid_loader)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, x5, labels in test_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}')\n",
    "\n",
    "input_size_1 = len(df['king_safety_tensor'][0])\n",
    "input_size_2 = len(df['material_tensor'][0])\n",
    "input_size_3 = len(df['space_tensor'][0])\n",
    "input_size_4 = len(df['pawn_structure_tensor'][0])\n",
    "input_size_5 = len(df['development_tensor'][0])\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_names = ['Adam', 'SGD', 'AdamW', 'RMSprop']\n",
    "optimizer_performance = {}\n",
    "\n",
    "for optimizer_name in optimizer_names:\n",
    "    print(f\"\\nTesting Optimizer: {optimizer_name}\")\n",
    "    \n",
    "    model = MultiModalDeepChess(input_size_1, input_size_2, input_size_3, input_size_4, input_size_5).to(device)\n",
    "    # Initialize the optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer_name == 'AdamW':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss, valid_loss = train(model, train_loader, valid_loader, criterion, optimizer, num_epochs)\n",
    "    test(model, test_loader, criterion)\n",
    "\n",
    "    optimizer_performance[optimizer] = valid_loss\n",
    "\n",
    "best_optimizer = min(optimizer_performance, key=optimizer_performance.get)\n",
    "print(f\"Best Batch Size: {best_optimizer} with Validation Loss: {optimizer_performance[best_optimizer]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 6.3592, Valid Loss: 6.3073\n",
      "Epoch [2/50], Train Loss: 6.0879, Valid Loss: 6.2803\n",
      "Epoch [3/50], Train Loss: 5.9334, Valid Loss: 6.2717\n",
      "Epoch [4/50], Train Loss: 5.8052, Valid Loss: 5.9615\n",
      "Epoch [5/50], Train Loss: 5.6338, Valid Loss: 5.9162\n",
      "Epoch [6/50], Train Loss: 5.4669, Valid Loss: 5.8057\n",
      "Epoch [7/50], Train Loss: 5.2926, Valid Loss: 5.8304\n",
      "Epoch [8/50], Train Loss: 5.1093, Valid Loss: 5.7362\n",
      "Epoch [9/50], Train Loss: 4.8963, Valid Loss: 5.5618\n",
      "Epoch [10/50], Train Loss: 4.6936, Valid Loss: 5.6491\n",
      "Epoch [11/50], Train Loss: 4.4919, Valid Loss: 5.2786\n",
      "Epoch [12/50], Train Loss: 4.2893, Valid Loss: 5.2998\n",
      "Epoch [13/50], Train Loss: 4.1143, Valid Loss: 5.2321\n",
      "Epoch [14/50], Train Loss: 3.9487, Valid Loss: 5.2120\n",
      "Epoch [15/50], Train Loss: 3.7752, Valid Loss: 5.0298\n",
      "Epoch [16/50], Train Loss: 3.6058, Valid Loss: 4.9331\n",
      "Epoch [17/50], Train Loss: 3.4709, Valid Loss: 4.9460\n",
      "Epoch [18/50], Train Loss: 3.3435, Valid Loss: 4.7750\n",
      "Epoch [19/50], Train Loss: 3.2120, Valid Loss: 4.6457\n",
      "Epoch [20/50], Train Loss: 3.0993, Valid Loss: 4.5447\n",
      "Epoch [21/50], Train Loss: 2.9843, Valid Loss: 4.5190\n",
      "Epoch [22/50], Train Loss: 2.8545, Valid Loss: 4.5712\n",
      "Epoch [23/50], Train Loss: 2.8000, Valid Loss: 4.3380\n",
      "Epoch [24/50], Train Loss: 2.6914, Valid Loss: 4.2296\n",
      "Epoch [25/50], Train Loss: 2.6380, Valid Loss: 4.2407\n",
      "Epoch [26/50], Train Loss: 2.5556, Valid Loss: 4.2182\n",
      "Epoch [27/50], Train Loss: 2.4785, Valid Loss: 4.0528\n",
      "Epoch [28/50], Train Loss: 2.4344, Valid Loss: 4.1123\n",
      "Epoch [29/50], Train Loss: 2.3267, Valid Loss: 4.0389\n",
      "Epoch [30/50], Train Loss: 2.3226, Valid Loss: 4.3085\n",
      "Epoch [31/50], Train Loss: 2.2322, Valid Loss: 4.0116\n",
      "Epoch [32/50], Train Loss: 2.1927, Valid Loss: 4.0128\n",
      "Epoch [33/50], Train Loss: 2.1198, Valid Loss: 3.9999\n",
      "Epoch [34/50], Train Loss: 2.1159, Valid Loss: 4.0517\n",
      "Epoch [35/50], Train Loss: 2.0498, Valid Loss: 3.9309\n",
      "Epoch [36/50], Train Loss: 1.9968, Valid Loss: 3.8440\n",
      "Epoch [37/50], Train Loss: 1.9583, Valid Loss: 3.8053\n",
      "Epoch [38/50], Train Loss: 1.9310, Valid Loss: 3.8591\n",
      "Epoch [39/50], Train Loss: 1.8757, Valid Loss: 3.7910\n",
      "Epoch [40/50], Train Loss: 1.8545, Valid Loss: 3.7252\n",
      "Epoch [41/50], Train Loss: 1.8293, Valid Loss: 3.7120\n",
      "Epoch [42/50], Train Loss: 1.7833, Valid Loss: 3.5778\n",
      "Epoch [43/50], Train Loss: 1.7423, Valid Loss: 3.6965\n",
      "Epoch [44/50], Train Loss: 1.7000, Valid Loss: 3.6361\n",
      "Epoch [45/50], Train Loss: 1.6981, Valid Loss: 3.7321\n",
      "Epoch [46/50], Train Loss: 1.6669, Valid Loss: 3.6167\n",
      "Epoch [47/50], Train Loss: 1.6395, Valid Loss: 3.5839\n",
      "Epoch [48/50], Train Loss: 1.6267, Valid Loss: 3.5987\n",
      "Epoch [49/50], Train Loss: 1.5654, Valid Loss: 3.5495\n",
      "Epoch [50/50], Train Loss: 1.5775, Valid Loss: 3.4787\n",
      "Test Loss: 3.3287\n"
     ]
    }
   ],
   "source": [
    "# Multi modal (global, piece)\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.df.iloc[idx]['global_tensor']\n",
    "        x2 = self.df.iloc[idx]['piece_tensor']\n",
    "        label = self.df.iloc[idx]['evaluation']\n",
    "\n",
    "        return x1, x2, torch.tensor(label, dtype=torch.float32)\n",
    "    \n",
    "train_dataset = MultiModalDataset(train_df)\n",
    "valid_dataset = MultiModalDataset(valid_df)\n",
    "test_dataset = MultiModalDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class MMDBN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MMDBN, self).__init__()\n",
    "        self.mmdbn = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mmdbn(x)\n",
    "    \n",
    "class MultiModalDeepChess(nn.Module):\n",
    "    def __init__(self, input_size_1, input_size_2):\n",
    "        super(MultiModalDeepChess, self).__init__()\n",
    "        self.branch_1 = MMDBN(input_size_1)\n",
    "        self.branch_2 = MMDBN(input_size_2)\n",
    "\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.branch_1(x1)\n",
    "        x2 = self.branch_2(x2)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        return self.merge_layers(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_best_loss(file_path=\"../output/best_loss.txt\"):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read().strip()\n",
    "            if content == \"\":\n",
    "                return float(\"inf\")\n",
    "            return float(content)\n",
    "        \n",
    "    return float(\"inf\")\n",
    "\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, epochs):\n",
    "    best_valid_loss = load_best_loss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for x1, x2, labels in train_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x1, x2)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        valid_loss = validate(model, valid_loader, criterion)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Valid Loss: {valid_loss:.4f}')\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"../output/model.pt\")\n",
    "\n",
    "            with open(\"../output/best_loss.txt\", \"w\") as f:\n",
    "                f.write(str(best_valid_loss))\n",
    "\n",
    "            print(f'Model saved with Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "def validate(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, labels in valid_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    return valid_loss / len(valid_loader)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, labels in test_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}')\n",
    "\n",
    "input_size_1 = len(df['global_tensor'][0])\n",
    "input_size_2 = len(df['piece_tensor'][0])\n",
    "model = MultiModalDeepChess(input_size_1, input_size_2).to(device)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, train_loader, valid_loader, criterion, optimizer, num_epochs)\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 1.9483, Valid Loss: 1.7715\n",
      "Epoch [2/50], Train Loss: 1.6397, Valid Loss: 1.6253\n",
      "Epoch [3/50], Train Loss: 1.5499, Valid Loss: 1.6132\n",
      "Epoch [4/50], Train Loss: 1.4832, Valid Loss: 1.6523\n",
      "Epoch [5/50], Train Loss: 1.4661, Valid Loss: 1.5201\n",
      "Epoch [6/50], Train Loss: 1.4683, Valid Loss: 1.4684\n",
      "Epoch [7/50], Train Loss: 1.4182, Valid Loss: 1.4734\n",
      "Epoch [8/50], Train Loss: 1.3988, Valid Loss: 1.4546\n",
      "Epoch [9/50], Train Loss: 1.4020, Valid Loss: 1.4644\n",
      "Epoch [10/50], Train Loss: 1.3923, Valid Loss: 1.4339\n",
      "Epoch [11/50], Train Loss: 1.3558, Valid Loss: 1.4311\n",
      "Epoch [12/50], Train Loss: 1.3328, Valid Loss: 1.4718\n",
      "Epoch [13/50], Train Loss: 1.3329, Valid Loss: 1.4261\n",
      "Epoch [14/50], Train Loss: 1.3035, Valid Loss: 1.4180\n",
      "Epoch [15/50], Train Loss: 1.2905, Valid Loss: 1.3657\n",
      "Epoch [16/50], Train Loss: 1.2715, Valid Loss: 1.3994\n",
      "Epoch [17/50], Train Loss: 1.2668, Valid Loss: 1.3516\n",
      "Epoch [18/50], Train Loss: 1.2466, Valid Loss: 1.3562\n",
      "Epoch [19/50], Train Loss: 1.2361, Valid Loss: 1.3436\n",
      "Epoch [20/50], Train Loss: 1.2161, Valid Loss: 1.3986\n",
      "Epoch [21/50], Train Loss: 1.1983, Valid Loss: 1.3294\n",
      "Epoch [22/50], Train Loss: 1.1912, Valid Loss: 1.3220\n",
      "Epoch [23/50], Train Loss: 1.1803, Valid Loss: 1.3143\n",
      "Epoch [24/50], Train Loss: 1.1537, Valid Loss: 1.3335\n",
      "Epoch [25/50], Train Loss: 1.1458, Valid Loss: 1.2799\n",
      "Epoch [26/50], Train Loss: 1.1241, Valid Loss: 1.3825\n",
      "Epoch [27/50], Train Loss: 1.1181, Valid Loss: 1.3283\n",
      "Epoch [28/50], Train Loss: 1.0937, Valid Loss: 1.3073\n",
      "Epoch [29/50], Train Loss: 1.0796, Valid Loss: 1.2641\n",
      "Epoch [30/50], Train Loss: 1.0581, Valid Loss: 1.2446\n",
      "Epoch [31/50], Train Loss: 1.0616, Valid Loss: 1.2579\n",
      "Epoch [32/50], Train Loss: 1.0564, Valid Loss: 1.2398\n",
      "Epoch [33/50], Train Loss: 1.0358, Valid Loss: 1.2545\n",
      "Epoch [34/50], Train Loss: 1.0390, Valid Loss: 1.2833\n",
      "Epoch [35/50], Train Loss: 1.0164, Valid Loss: 1.2081\n",
      "Epoch [36/50], Train Loss: 0.9919, Valid Loss: 1.1999\n",
      "Epoch [37/50], Train Loss: 0.9845, Valid Loss: 1.2617\n",
      "Epoch [38/50], Train Loss: 0.9818, Valid Loss: 1.1844\n",
      "Epoch [39/50], Train Loss: 0.9580, Valid Loss: 1.2235\n",
      "Epoch [40/50], Train Loss: 0.9666, Valid Loss: 1.2333\n",
      "Epoch [41/50], Train Loss: 0.9475, Valid Loss: 1.1796\n",
      "Epoch [42/50], Train Loss: 0.9268, Valid Loss: 1.1924\n",
      "Epoch [43/50], Train Loss: 0.9338, Valid Loss: 1.1765\n",
      "Epoch [44/50], Train Loss: 0.9082, Valid Loss: 1.1968\n",
      "Epoch [45/50], Train Loss: 0.9002, Valid Loss: 1.1669\n",
      "Epoch [46/50], Train Loss: 0.8815, Valid Loss: 1.1400\n",
      "Epoch [47/50], Train Loss: 0.8698, Valid Loss: 1.1404\n",
      "Epoch [48/50], Train Loss: 0.8557, Valid Loss: 1.1882\n",
      "Epoch [49/50], Train Loss: 0.8605, Valid Loss: 1.1381\n",
      "Epoch [50/50], Train Loss: 0.8458, Valid Loss: 1.1319\n",
      "Test Loss: 1.1228\n"
     ]
    }
   ],
   "source": [
    "# Multi modal (global, piece), batch size 512\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.df.iloc[idx]['global_tensor']\n",
    "        x2 = self.df.iloc[idx]['piece_tensor']\n",
    "        label = self.df.iloc[idx]['evaluation']\n",
    "\n",
    "        return x1, x2, torch.tensor(label, dtype=torch.float32)\n",
    "    \n",
    "train_dataset = MultiModalDataset(train_df)\n",
    "valid_dataset = MultiModalDataset(valid_df)\n",
    "test_dataset = MultiModalDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=512, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "class MMDBN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MMDBN, self).__init__()\n",
    "        self.mmdbn = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mmdbn(x)\n",
    "    \n",
    "class MultiModalDeepChess(nn.Module):\n",
    "    def __init__(self, input_size_1, input_size_2):\n",
    "        super(MultiModalDeepChess, self).__init__()\n",
    "        self.branch_1 = MMDBN(input_size_1)\n",
    "        self.branch_2 = MMDBN(input_size_2)\n",
    "\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.branch_1(x1)\n",
    "        x2 = self.branch_2(x2)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        return self.merge_layers(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_best_loss(file_path=\"../output/best_loss.txt\"):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read().strip()\n",
    "            if content == \"\":\n",
    "                return float(\"inf\")\n",
    "            return float(content)\n",
    "        \n",
    "    return float(\"inf\")\n",
    "\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, epochs):\n",
    "    best_valid_loss = load_best_loss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for x1, x2, labels in train_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x1, x2)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        valid_loss = validate(model, valid_loader, criterion)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Valid Loss: {valid_loss:.4f}')\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"../output/model.pt\")\n",
    "\n",
    "            with open(\"../output/best_loss.txt\", \"w\") as f:\n",
    "                f.write(str(best_valid_loss))\n",
    "\n",
    "            print(f'Model saved with Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "def validate(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, labels in valid_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    return valid_loss / len(valid_loader)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, labels in test_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}')\n",
    "\n",
    "input_size_1 = len(df['global_tensor'][0])\n",
    "input_size_2 = len(df['piece_tensor'][0])\n",
    "model = MultiModalDeepChess(input_size_1, input_size_2).to(device)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, train_loader, valid_loader, criterion, optimizer, num_epochs)\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 6.8681, Valid Loss: 7.0359, Learning Rate: [0.9755282581475768]\n",
      "Epoch [2/50], Train Loss: 6.2489, Valid Loss: 6.3166, Learning Rate: [0.9045084971874737]\n",
      "Epoch [3/50], Train Loss: 6.1128, Valid Loss: 6.7162, Learning Rate: [0.7938926261462366]\n",
      "Epoch [4/50], Train Loss: 6.0248, Valid Loss: 6.1066, Learning Rate: [0.6545084971874737]\n",
      "Epoch [5/50], Train Loss: 5.9311, Valid Loss: 6.2340, Learning Rate: [0.5]\n",
      "Epoch [6/50], Train Loss: 5.8370, Valid Loss: 6.0452, Learning Rate: [0.34549150281252633]\n",
      "Epoch [7/50], Train Loss: 5.7399, Valid Loss: 6.0683, Learning Rate: [0.2061073738537635]\n",
      "Epoch [8/50], Train Loss: 5.6219, Valid Loss: 5.9177, Learning Rate: [0.09549150281252633]\n",
      "Epoch [9/50], Train Loss: 5.5191, Valid Loss: 5.8893, Learning Rate: [0.024471741852423234]\n",
      "Epoch [10/50], Train Loss: 5.4435, Valid Loss: 5.8784, Learning Rate: [0.0]\n",
      "Epoch [11/50], Train Loss: 5.4152, Valid Loss: 5.8784, Learning Rate: [0.024471741852423234]\n",
      "Epoch [12/50], Train Loss: 5.4220, Valid Loss: 5.8801, Learning Rate: [0.0954915028125267]\n",
      "Epoch [13/50], Train Loss: 5.4406, Valid Loss: 5.9003, Learning Rate: [0.2061073738537643]\n",
      "Epoch [14/50], Train Loss: 5.4965, Valid Loss: 5.9177, Learning Rate: [0.3454915028125278]\n",
      "Epoch [15/50], Train Loss: 5.5619, Valid Loss: 6.0003, Learning Rate: [0.5000000000000021]\n",
      "Epoch [16/50], Train Loss: 5.6305, Valid Loss: 5.9190, Learning Rate: [0.6545084971874765]\n",
      "Epoch [17/50], Train Loss: 5.6972, Valid Loss: 5.9950, Learning Rate: [0.7938926261462399]\n",
      "Epoch [18/50], Train Loss: 5.7199, Valid Loss: 6.2682, Learning Rate: [0.9045084971874777]\n",
      "Epoch [19/50], Train Loss: 5.7321, Valid Loss: 6.0870, Learning Rate: [0.975528258147581]\n",
      "Epoch [20/50], Train Loss: 5.7234, Valid Loss: 6.1959, Learning Rate: [1.0000000000000042]\n",
      "Epoch [21/50], Train Loss: 5.6881, Valid Loss: 5.9706, Learning Rate: [0.9755282581475809]\n",
      "Epoch [22/50], Train Loss: 5.6195, Valid Loss: 5.9521, Learning Rate: [0.9045084971874778]\n",
      "Epoch [23/50], Train Loss: 5.5008, Valid Loss: 5.9944, Learning Rate: [0.79389262614624]\n",
      "Epoch [24/50], Train Loss: 5.3854, Valid Loss: 5.8082, Learning Rate: [0.6545084971874765]\n",
      "Epoch [25/50], Train Loss: 5.2304, Valid Loss: 5.8096, Learning Rate: [0.5000000000000021]\n",
      "Epoch [26/50], Train Loss: 5.0133, Valid Loss: 5.6065, Learning Rate: [0.34549150281252783]\n",
      "Epoch [27/50], Train Loss: 4.7818, Valid Loss: 5.5463, Learning Rate: [0.20610737385376438]\n",
      "Epoch [28/50], Train Loss: 4.5221, Valid Loss: 5.4031, Learning Rate: [0.09549150281252677]\n",
      "Epoch [29/50], Train Loss: 4.2744, Valid Loss: 5.2276, Learning Rate: [0.02447174185242339]\n",
      "Epoch [30/50], Train Loss: 4.0918, Valid Loss: 5.2094, Learning Rate: [0.0]\n",
      "Epoch [31/50], Train Loss: 4.0225, Valid Loss: 5.2094, Learning Rate: [0.024471741852423234]\n",
      "Epoch [32/50], Train Loss: 4.0374, Valid Loss: 5.1974, Learning Rate: [0.09549150281252639]\n",
      "Epoch [33/50], Train Loss: 4.0870, Valid Loss: 5.1550, Learning Rate: [0.20610737385376374]\n",
      "Epoch [34/50], Train Loss: 4.1682, Valid Loss: 5.3246, Learning Rate: [0.34549150281252683]\n",
      "Epoch [35/50], Train Loss: 4.3379, Valid Loss: 5.2458, Learning Rate: [0.5000000000000009]\n",
      "Epoch [36/50], Train Loss: 4.5260, Valid Loss: 5.4608, Learning Rate: [0.654508497187475]\n",
      "Epoch [37/50], Train Loss: 4.7119, Valid Loss: 5.4577, Learning Rate: [0.7938926261462382]\n",
      "Epoch [38/50], Train Loss: 4.8507, Valid Loss: 5.5925, Learning Rate: [0.9045084971874756]\n",
      "Epoch [39/50], Train Loss: 4.9336, Valid Loss: 5.6368, Learning Rate: [0.9755282581475789]\n",
      "Epoch [40/50], Train Loss: 4.9412, Valid Loss: 5.6155, Learning Rate: [1.0000000000000024]\n",
      "Epoch [41/50], Train Loss: 4.9026, Valid Loss: 5.8976, Learning Rate: [0.9755282581475795]\n",
      "Epoch [42/50], Train Loss: 4.7836, Valid Loss: 5.7083, Learning Rate: [0.9045084971874762]\n",
      "Epoch [43/50], Train Loss: 4.5930, Valid Loss: 5.3812, Learning Rate: [0.7938926261462388]\n",
      "Epoch [44/50], Train Loss: 4.3207, Valid Loss: 5.4291, Learning Rate: [0.6545084971874765]\n",
      "Epoch [45/50], Train Loss: 4.0145, Valid Loss: 5.0596, Learning Rate: [0.5000000000000014]\n",
      "Epoch [46/50], Train Loss: 3.6500, Valid Loss: 5.0099, Learning Rate: [0.3454915028125274]\n",
      "Epoch [47/50], Train Loss: 3.2821, Valid Loss: 4.8145, Learning Rate: [0.20610737385376343]\n",
      "Epoch [48/50], Train Loss: 2.9225, Valid Loss: 4.4534, Learning Rate: [0.09549150281252668]\n",
      "Epoch [49/50], Train Loss: 2.6152, Valid Loss: 4.2973, Learning Rate: [0.02447174185242363]\n",
      "Epoch [50/50], Train Loss: 2.4223, Valid Loss: 4.2688, Learning Rate: [0.0]\n",
      "Test Loss: 4.1454\n"
     ]
    }
   ],
   "source": [
    "# Normalized, batch size 512, optimizer AdaDelta\n",
    "class MultiModalDeepChessDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.df.iloc[idx]['king_safety_tensor']\n",
    "        x2 = self.df.iloc[idx]['material_tensor']\n",
    "        x3 = self.df.iloc[idx]['space_tensor']\n",
    "        x4 = self.df.iloc[idx]['pawn_structure_tensor']\n",
    "        x5 = self.df.iloc[idx]['development_tensor']\n",
    "        label = self.df.iloc[idx]['evaluation']\n",
    "\n",
    "        return x1, x2, x3, x4, x5, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "train_dataset = MultiModalDeepChessDataset(train_df)\n",
    "valid_dataset = MultiModalDeepChessDataset(valid_df)\n",
    "test_dataset = MultiModalDeepChessDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class MMDBN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MMDBN, self).__init__()\n",
    "        self.mmdbn = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mmdbn(x)\n",
    "    \n",
    "class MultiModalDeepChess(nn.Module):\n",
    "    def __init__(self, input_size_1, input_size_2, input_size_3, input_size_4, input_size_5):\n",
    "        super(MultiModalDeepChess, self).__init__()\n",
    "        self.branch_1 = MMDBN(input_size_1)\n",
    "        self.branch_2 = MMDBN(input_size_2)\n",
    "        self.branch_3 = MMDBN(input_size_3)\n",
    "        self.branch_4 = MMDBN(input_size_4)\n",
    "        self.branch_5 = MMDBN(input_size_5)\n",
    "\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Linear(1000, 800),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(800, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4, x5):\n",
    "        x1 = self.branch_1(x1)\n",
    "        x2 = self.branch_2(x2)\n",
    "        x3 = self.branch_3(x3)\n",
    "        x4 = self.branch_4(x4)\n",
    "        x5 = self.branch_5(x5)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "\n",
    "        return self.merge_layers(x)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_best_loss(file_path=\"../output/best_loss.txt\"):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read().strip()\n",
    "            if content == \"\":\n",
    "                return float(\"inf\")\n",
    "            return float(content)\n",
    "        \n",
    "    return float(\"inf\")\n",
    "\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, epochs, scheduler):\n",
    "    best_valid_loss = load_best_loss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for x1, x2, x3, x4, x5, labels in train_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        valid_loss = validate(model, valid_loader, criterion)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Valid Loss: {valid_loss:.4f}, Learning Rate: {scheduler.get_last_lr()}')\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"../output/model.pt\")\n",
    "\n",
    "            with open(\"../output/best_loss.txt\", \"w\") as f:\n",
    "                f.write(str(best_valid_loss))\n",
    "\n",
    "            print(f'Model saved with Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "def validate(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, x5, labels in valid_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    return valid_loss / len(valid_loader)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, x5, labels in test_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2, x3, x4, x5)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}')\n",
    "\n",
    "input_size_1 = len(df['king_safety_tensor'][0])\n",
    "input_size_2 = len(df['material_tensor'][0])\n",
    "input_size_3 = len(df['space_tensor'][0])\n",
    "input_size_4 = len(df['pawn_structure_tensor'][0])\n",
    "input_size_5 = len(df['development_tensor'][0])\n",
    "num_epochs = 50\n",
    "model = MultiModalDeepChess(input_size_1, input_size_2, input_size_3, input_size_4, input_size_5).to(device)\n",
    "learning_rate = 1.0\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "train(model, train_loader, valid_loader, criterion, optimizer, num_epochs, scheduler)\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 5.2797, Valid Loss: 4.3695\n",
      "Epoch [2/50], Train Loss: 2.4254, Valid Loss: 2.4144\n",
      "Epoch [3/50], Train Loss: 1.0963, Valid Loss: 2.0789\n",
      "Epoch [4/50], Train Loss: 0.7416, Valid Loss: 1.9422\n",
      "Epoch [5/50], Train Loss: 0.6061, Valid Loss: 1.8650\n",
      "Epoch [6/50], Train Loss: 0.5009, Valid Loss: 1.8133\n",
      "Epoch [7/50], Train Loss: 0.4234, Valid Loss: 1.7987\n",
      "Epoch [8/50], Train Loss: 0.3625, Valid Loss: 1.7751\n",
      "Epoch [9/50], Train Loss: 0.3183, Valid Loss: 1.7702\n",
      "Epoch [10/50], Train Loss: 0.2846, Valid Loss: 1.6961\n",
      "Model saved with Validation Loss: 1.6961\n",
      "Epoch [11/50], Train Loss: 0.2567, Valid Loss: 1.6732\n",
      "Model saved with Validation Loss: 1.6732\n",
      "Epoch [12/50], Train Loss: 0.2412, Valid Loss: 1.7319\n",
      "Epoch [13/50], Train Loss: 0.2164, Valid Loss: 1.6665\n",
      "Model saved with Validation Loss: 1.6665\n",
      "Epoch [14/50], Train Loss: 0.1889, Valid Loss: 1.6249\n",
      "Model saved with Validation Loss: 1.6249\n",
      "Epoch [15/50], Train Loss: 0.1728, Valid Loss: 1.6253\n",
      "Epoch [16/50], Train Loss: 0.1586, Valid Loss: 1.6732\n",
      "Epoch [17/50], Train Loss: 0.1575, Valid Loss: 1.6312\n",
      "Epoch [18/50], Train Loss: 0.1425, Valid Loss: 1.6458\n",
      "Epoch [19/50], Train Loss: 0.1236, Valid Loss: 1.6123\n",
      "Model saved with Validation Loss: 1.6123\n",
      "Epoch [20/50], Train Loss: 0.1256, Valid Loss: 1.6105\n",
      "Model saved with Validation Loss: 1.6105\n",
      "Epoch [21/50], Train Loss: 0.1022, Valid Loss: 1.6708\n",
      "Epoch [22/50], Train Loss: 0.1142, Valid Loss: 1.7067\n",
      "Epoch [23/50], Train Loss: 0.1168, Valid Loss: 1.6580\n",
      "Epoch [24/50], Train Loss: 0.0995, Valid Loss: 1.6205\n",
      "Epoch [25/50], Train Loss: 0.1000, Valid Loss: 1.5910\n",
      "Model saved with Validation Loss: 1.5910\n",
      "Epoch [26/50], Train Loss: 0.0899, Valid Loss: 1.6527\n",
      "Epoch [27/50], Train Loss: 0.0965, Valid Loss: 1.6299\n",
      "Epoch [28/50], Train Loss: 0.1012, Valid Loss: 1.5760\n",
      "Model saved with Validation Loss: 1.5760\n",
      "Epoch [29/50], Train Loss: 0.0761, Valid Loss: 1.6274\n",
      "Epoch [30/50], Train Loss: 0.0690, Valid Loss: 1.6076\n",
      "Epoch [31/50], Train Loss: 0.0814, Valid Loss: 1.6280\n",
      "Epoch [32/50], Train Loss: 0.0825, Valid Loss: 1.5537\n",
      "Model saved with Validation Loss: 1.5537\n",
      "Epoch [33/50], Train Loss: 0.0869, Valid Loss: 1.6534\n",
      "Epoch [34/50], Train Loss: 0.0716, Valid Loss: 1.5868\n",
      "Epoch [35/50], Train Loss: 0.0628, Valid Loss: 1.6575\n",
      "Epoch [36/50], Train Loss: 0.0724, Valid Loss: 1.6188\n",
      "Epoch [37/50], Train Loss: 0.0635, Valid Loss: 1.5732\n",
      "Epoch [38/50], Train Loss: 0.0540, Valid Loss: 1.5938\n",
      "Epoch [39/50], Train Loss: 0.0510, Valid Loss: 1.6329\n",
      "Epoch [40/50], Train Loss: 0.0709, Valid Loss: 1.6405\n",
      "Epoch [41/50], Train Loss: 0.0650, Valid Loss: 1.6267\n",
      "Epoch [42/50], Train Loss: 0.0532, Valid Loss: 1.6268\n",
      "Epoch [43/50], Train Loss: 0.0608, Valid Loss: 1.5910\n",
      "Epoch [44/50], Train Loss: 0.0487, Valid Loss: 1.6511\n",
      "Epoch [45/50], Train Loss: 0.0586, Valid Loss: 1.6009\n",
      "Epoch [46/50], Train Loss: 0.0630, Valid Loss: 1.6296\n",
      "Epoch [47/50], Train Loss: 0.0578, Valid Loss: 1.5834\n",
      "Epoch [48/50], Train Loss: 0.0476, Valid Loss: 1.6004\n",
      "Epoch [49/50], Train Loss: 0.0458, Valid Loss: 1.5966\n",
      "Epoch [50/50], Train Loss: 0.0496, Valid Loss: 1.6425\n",
      "Test Loss: 1.6361\n"
     ]
    }
   ],
   "source": [
    "# Baseline\n",
    "class BaselineDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.df.iloc[idx]['bitstring_tensor']\n",
    "        x2 = self.df.iloc[idx]['bitstring_tensor']\n",
    "        label = self.df.iloc[idx]['evaluation']\n",
    "\n",
    "        return x1, x2, torch.tensor(label, dtype=torch.float32)\n",
    "    \n",
    "train_dataset = BaselineDataset(train_df)\n",
    "valid_dataset = BaselineDataset(valid_df)\n",
    "test_dataset = BaselineDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class BaselineDBN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BaselineDBN, self).__init__()\n",
    "        self.baselinedbn = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.baselinedbn(x)\n",
    "    \n",
    "class BaselineDeepChess(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BaselineDeepChess, self).__init__()\n",
    "        self.branch_1 = BaselineDBN(input_size)\n",
    "        self.branch_2 = BaselineDBN(input_size)\n",
    "\n",
    "        self.merge_layers = nn.Sequential(\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.branch_1(x1)\n",
    "        x2 = self.branch_2(x2)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        return self.merge_layers(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_best_loss(file_path=\"../output/best_loss.txt\"):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read().strip()\n",
    "            if content == \"\":\n",
    "                return float(\"inf\")\n",
    "            return float(content)\n",
    "        \n",
    "    return float(\"inf\")\n",
    "\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, epochs):\n",
    "    best_valid_loss = load_best_loss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for x1, x2, labels in train_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x1, x2)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        valid_loss = validate(model, valid_loader, criterion)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Valid Loss: {valid_loss:.4f}')\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"../output/model.pt\")\n",
    "\n",
    "            with open(\"../output/best_loss.txt\", \"w\") as f:\n",
    "                f.write(str(best_valid_loss))\n",
    "\n",
    "            print(f'Model saved with Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "def validate(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, labels in valid_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    return valid_loss / len(valid_loader)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, labels in test_loader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            labels = labels.view(-1, 1).to(device).float()\n",
    "\n",
    "            output = model(x1, x2)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}')\n",
    "\n",
    "input_size = len(df['bitstring_tensor'][0])\n",
    "model = BaselineDeepChess(input_size).to(device)  # bitstring only\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, train_loader, valid_loader, criterion, optimizer, num_epochs)\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "evaluations = df['evaluation']\n",
    "sns.boxplot(evaluations)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.46 14.05\n"
     ]
    }
   ],
   "source": [
    "print(evaluations.min(), evaluations.max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
